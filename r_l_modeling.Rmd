---
title: "R/L across languages: Modeling"
author: "Aleksandra Ä†wiek"
date: "2023-10-26"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float: yes
    df_print: paged
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_notebook:
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction:

This script uses the output of "r_l_preparation.Rmd" and returns all values reported in the paper.

# Setup:

Load packages:

```{r, message = FALSE, warning = FALSE}
library(tidyverse) # data processing
library(brms) # bayesian models
library(cmdstanr)
library(ggdist) # for plotting
# option for Bayesian regression models:
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())

# Set the script's path as working directory
parentfolder = rstudioapi::getActiveDocumentContext()$path 
setwd(dirname(parentfolder))
parentfolder <- getwd()

models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
data          <- paste0(parentfolder, '/data/')
```

For reproducible reporting:

```{r, message = FALSE, warning = FALSE}
packageVersion('tidyverse')
packageVersion('brms')
packageVersion('cmdstanr')
packageVersion('ggdist')
```

Load ggplot2 theme:

```{r}
source('theme_timo.R')
```

Load data:

```{r, message = FALSE, warning = FALSE}
web <- read_csv(paste0(data, 'web_experiment_cleaned.csv'))
web_raw <- read_csv(paste0(data, '/web_raw_trials.csv'))
```

# Online experiment

## Descriptive statistics

First, how many participants?

```{r}
nrow(web)
```

First, how many languages?

```{r}
web %>% count(Language) %>% nrow()
```

How many families?

```{r}
web %>% count(Family) %>% nrow()
```


How many non-R/L-distinct L1 among the languages?

```{r}
web %>% count(Language, L1_distinction) %>% count(L1_distinction)
```


What is the grand average congruent behavior?

```{r}
mean(web$Match)
```

87%

What about only among those who have L1 without the distinction?

```{r}
web %>%
  filter(L1_distinction == "no") %>%
  summarize(mean_match = mean(Match, na.rm = TRUE))
```

84%

What about only among those who have L1 without the distinction and no L2 that distinguishes?

```{r}
web %>%
  filter(L1_distinction == 'no') %>%
  filter(!EnglishL2YesNo) %>% 
  filter(no_dist_L2 == 'FALSE') %>% 
  summarize(mean_match = mean(Match, na.rm = TRUE))
```

Also 84%

Compute average matching behavior per language and sort:

```{r}
web_avg <- web %>%
  group_by(Language) %>% 
  summarize(M = mean(Match)) %>% 
  arrange(desc(M)) %>% 
  mutate(percent = round(M, 2) * 100,
         percent = str_c(percent, '%'))

# Show:

web_avg %>% print(n = Inf)
```

Check some demographics, also to report in the paper. First, the number of participants per language:

```{r}
web %>% 
  count(Name, sort = TRUE) %>% print(n = Inf)
```


Then, the number of L1 speakers who have R/L distinction vs. who don't (we can think if we put the approximant and trill together):

```{r}
web %>% count(L1_distinction) %>%
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

How many people are not bilinguals?

```{r}
sum(is.na(web$L2)) / nrow(web)

# bilinguals:

1 - sum(is.na(web$L2)) / nrow(web)
```

Check how many people knew English as their L2:

```{r}
web %>% count(EnglishL2YesNo) %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

Of those that don't use a R/L distinction in their L1, how many use R/L distinction in their L2? (double-check if logic alright!)

```{r}
web %>%
  filter(L1_distinction == 'no') %>% 
  count(no_dist_L2 == FALSE) %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

How many "pure" speakers were there?, i.e., those people that 1) don't know English, 2) don't use an L1 with a R/L distinction, and 3) don't know an L2 that distinguishes R/L.

```{r}
web %>% 
  filter(L1_distinction == 'no') %>%  # excludes English as well
  filter(!EnglishL2YesNo) %>% 
  filter(no_dist_L2 == 'FALSE') %>% 
  nrow()
```

43 people.

Let's check if this is correct. This gives the list of all participants for whom this applies.

```{r}
web %>% 
    filter(L1_distinction == 'no' & !EnglishL2YesNo & no_dist_L2 == FALSE) %>% 
    print(n = 50)
```

Are these really "pure"? What languages do they speak?

```{r}
web %>% 
  filter(L1_distinction == 'no') %>%  # excludes English as well
  filter(!EnglishL2YesNo) %>% 
  filter(no_dist_L2 == 'FALSE') %>% 
  count(Language)
```

Mostly Chinese and Japanese.


Nevertheless, let's explore whether these also show kiki/bouba matches?

```{r}
web %>% 
  filter(L1_distinction == 'no') %>%  # excludes English as well
  filter(!EnglishL2YesNo) %>% 
  filter(no_dist_L2 == 'FALSE') %>% 
  count(Match) %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

Yes, as shown above 84%.

## Main model:


Set parameters that will be used for all MCMC sampling:

```{r}
mywarmup <- 4000
myiter <- 6000
```

Check the distribution of scripts across families to make decisions about random effects structure:

```{r}
table(web$Family, web$L1_distinction)
```


Let's factor code the Order and L1_distinction effects:

```{r}
web <- mutate(web,
               Order = factor(Order, levels = c('r_first', 'l_first')),
               L1_distinction = factor(L1_distinction, levels = c('no', 'approximant', 'trill')))
```

Check that the order effect is approximately balanced:

```{r}
web %>% count(Order) %>%
  mutate(prop = n / sum(n))
```

I omit some things for now... Because for now, L1_distinction is three-way.

```{r}
web <- mutate(web,
               order_num = ifelse(Order == 'r_first', -0.5, +0.5))
```

The main model with Order and L1_distinction fixed effects, as well as Language and Family random effects.

```{r, message = FALSE, warning = FALSE}
web_mdl <- brm(Match ~ 1 + order_num + L1_distinction +
                 (1 + order_num|Language) + 
                 (1 + order_num|Family),
                data = web,
                family = bernoulli(link = 'logit'),
                backend = "cmdstanr",
                seed = 42,
                cores = 4,
                iter = 2000,
                warmup = 1000,
                control = list(adapt_delta = 0.995,
                               max_treedepth = 13),
                file = paste0(models, "web_mdl.rds"))
```

Show the model:

```{r}
web_mdl
```


Compute grand averages with 95% lower and upper bound of credible interval to report:

```{r}
plogis(fixef(web_mdl)[1, ][1])
plogis(fixef(web_mdl)[1, ][3]) # lwr bound
plogis(fixef(web_mdl)[1, ][4]) # upr bound
```

Perform hypothesis tests:

```{r}
hypothesis(web_mdl, 'Intercept > 0')
hypothesis(web_mdl, 'L1_distinctionapproximant > 0')
hypothesis(web_mdl, 'L1_distinctiontrill > 0')
hypothesis(web_mdl, 'order_num > 0')
```

Check the descriptive averages for script and order:

```{r}
web %>% 
  group_by(L1_distinction) %>% 
  summarize(prop = mean(Match)) %>% 
  mutate(percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```


```{r}
web %>% 
  group_by(Order) %>% 
  summarize(prop = mean(Match)) %>% 
  mutate(percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```


Extract the posteriors:

```{r}
posts <- posterior_samples(web_mdl)
posts <- select(posts,
                b_Intercept:b_L1_distinctiontrill)
```


Make the posteriors into long format for plotting (Figure 3):

```{r}
posts <- pivot_longer(posts,
                      cols = b_Intercept:b_L1_distinctiontrill,
                      names_to = 'coefficient',
                      values_to = 'posterior')
```

Change names for plotting and order:

```{r}
posts <- posts %>%
  mutate(coefficient = ifelse(coefficient == 'b_Intercept', 'Intercept', coefficient),
         coefficient = ifelse(coefficient == 'b_order_num', 'Order', coefficient),
         coefficient = ifelse(coefficient == 'b_L1_distinctionapproximant', 'L1 approximant', coefficient),
         coefficient = ifelse(coefficient == 'b_L1_distinctiontrill', 'L1 trill', coefficient),
         coefficient = factor(coefficient,
                              levels = rev(c('Intercept', 'Order', 'L1 approximant', 'L1 trill'))))
```

Make a plot of this:

```{r, fig.width = 8, fig.height = 7}
# Aesthetics and geom:

p <- posts %>% ggplot(aes(y = coefficient, x = posterior)) +
  geom_vline(aes(xintercept = 0), linetype = 2) +
  stat_halfeye(alpha = 0.5, fill = 'steelblue')

# Axis labels and coordinates:

p <- p +
  coord_cartesian(xlim = c(-4, +6)) +
  scale_x_continuous(breaks = seq(-4, 6, 1)) +
  xlab('Coefficient') +
  ylab(NULL)

# Tweak cosmetics:

p <- p +
  theme_timo + 
  theme(axis.title.x = element_text(margin = margin(t = 12, b = 0,
                                                    r = 0, l = 0),
                                    face = 'bold', size = 14),
        axis.text.x = element_text(face = 'bold', size = 12),
        axis.text.y = element_text(face = 'bold', size = 12))

# Show and save:

p
ggsave(plot = p, filename = paste0(plots, 'main_model_coefficients.pdf'),
       width = 8, height = 6)
```


For Figure 4, we want to combine the predictions for each language with the averages of each language. First, we need to get the predictions from the model:

```{r}
# Setup data frame with predictors to get predictions for:
newdata <- data.frame(Language = unique(web$Language))
newdata$order_num <- 0
newdata$L1_distinction <- web[match(newdata$Language, web$Language), ]$L1_distinction
newdata$Family <- web[match(newdata$Language, web$Language), ]$Family

# Get predictions and append to dataframe:

fit <- fitted(web_mdl, newdata = newdata,
              re_formula = NULL, robust = TRUE)
colnames(fit) = c('fit', 'se', 'lwr', 'upr')
newdata <- cbind(newdata, fit)

# Order predictions by descriptive average:

newdata <- arrange(newdata, fit)
newdata <- mutate(newdata,
                  Language = factor(Language, levels = newdata$Language))
```

How many languages are over 0.5? (will be reported in paper)

```{r}
sum(newdata$lwr > 0.5)
```

Finally, add the averages to the plot:

```{r}
newdata$avg <- web_avg[match(newdata$Language, web_avg$Language), ]$M
```

Match language names into there and order:

```{r}
newdata$Language <- web[match(newdata$Language, web$Language), ]$Name
newdata[newdata$Language == 'Chinese', ]$Language <- 'Mandarin Chinese'

newdata <- mutate(newdata,
                  Language = factor(Language, levels = newdata$Language))
```

Setup the plot:

```{r, fig.width = 8, fig.height = 6}
# Aesthetics and geom:

p <- newdata %>% 
  ggplot(aes(x = Language, col = L1_distinction, y = fit,
      ymin = lwr, ymax = upr)) +
  geom_errorbar(aes(col = L1_distinction),
                size = 1.6, width = 0.6) +
  geom_point(size = 9, shape = 15) +
  geom_hline(yintercept = 0.5, linetype = 2, size = 1.5, col = 'grey') + 
  geom_point(aes(y = avg), col = 'black', shape = 23, size = 8.5,
             fill = 'white', stroke = 1.5)
  
# Axis labels:

p <- p +
  labs(x = '', y = 'Proportion\nof congruent responses') +
  ggtitle('Posterior medians and descriptive averages of\ncongruent responses by language and R/L contrast') 

# Tweak cosmetics:

p <- p +
  #scale_color_manual(values = c('#9C1437', '#355691')) + # ZAS-COLORS
  theme_timo + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, face = 'bold',
                                   size = 30),
        axis.text.y = element_text(face = 'bold', size = 24),
        axis.title = element_text(face = 'bold', size = 40),
        axis.title.y = element_text(face = 'bold', size = 40,
                                    margin = margin(t = 0, r = 35,
                                                    b = 0, l = 0)),
        plot.title = element_text(face = 'bold', size = 40,
                                  margin = margin(t = 0, r = 0,
                                                  b = 30, l = 0)),
        legend.text = element_text(size = 30),
        legend.title = element_blank(),
        legend.position = c(0.95, 0.15),
        legend.justification = c('right', 'bottom'))

# Show and save:

p
ggsave(plot = p, filename = paste0(plots, 'by_language_results.pdf'),
       width = 28, height = 12)
```





