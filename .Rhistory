web <- rename(web,
L1_raw = l1.x,
L2_raw = l2.x,
L1_cleaned = l1.y,
L2_cleaned = l2.y)
# Chunk 26: how many per participant
ppt_N <- web %>% count(ID)
# Chunk 27: check response N
all(ppt_N$n == 2)
# Chunk 28: check who
filter(ppt_N, n != 2)
# Chunk 29: remove less than 2
# Vector of participants to exclude:
excludes <- filter(ppt_N, n != 2) %>% pull(ID)
# Exclude:
web <- filter(web, !(ID %in% excludes))
rm(excludes)
# Chunk 30: counts per lang
web %>% count(Language) %>%
mutate(n = n / 2) %>%
print()
# Chunk 31: remove too few
web <- filter(web, Language != "MS")
web <- filter(web, Language != "TA")
# Chunk 32: sort l1
web <- filter(web,
!L1_cleaned %in% c('arabic',
'kurdish', 'pashto',
'wolof'))
# Close enough for our purposes from the perspective of kiki/bouba and the fact that language families don't cross â€” sorry if you are a speaker of one of these languages as we recognize the difference, but the most important thing is that these do not exert a bias from a "macro perspective" of language families:
web[web$L1_cleaned == 'dutch', ]$L1_cleaned <- 'german'
web[web$L1_cleaned == 'czech', ]$L1_cleaned <- 'polish'
web[web$L1_cleaned == 'latvian', ]$L1_cleaned <- 'polish'
# Chunk 33: clean l1
table(web$L1_cleaned, web$Language)
web[web$L1_cleaned == 'russian', ]$Language <- 'RU'
web[web$L1_cleaned == 'armenian', ]$Language <- 'AM'
web[web$L1_cleaned == 'chinese', ]$Language <- 'CN'
web[web$L1_cleaned == 'english', ]$Language <- 'EN'
web[web$L1_cleaned == 'estonian', ]$Language <- 'EE'
web[web$L1_cleaned == 'german', ]$Language <- 'DE'
web[web$L1_cleaned == 'finnish', ]$Language <- 'FI'
web[web$L1_cleaned == 'french', ]$Language <- 'FR'
web[web$L1_cleaned == 'italian', ]$Language <- 'IT'
web[web$L1_cleaned == 'korean', ]$Language <- 'KR'
web[web$L1_cleaned == 'polish', ]$Language <- 'PL'
web[web$L1_cleaned == 'zulu', ]$Language <- 'ZU'
web[web$L1_cleaned == 'spanish', ]$Language <- 'ES'
web[web$L1_cleaned == 'swedish', ]$Language <- 'SE'
web[web$L1_cleaned == 'portuguese', ]$Language <- 'PT'
web[web$L1_cleaned == 'thai', ]$Language <- 'TH'
# Chunk 34: load lang file
langs <- read_csv(paste0(data, 'language_info.csv'))
# Chunk 35: merge lang
web <- left_join(web, langs)
# Chunk 36: add resp
web <- rename(web,
Resp = inputvalue)
# Chunk 37: add condition
web <- mutate(web,
Condition = str_replace(audio, '\\.wav', ''))
# Chunk 38: congruency
web$match = web$Condition == web$Resp
# Chunk 39: trial
web$sound <- xfun::sans_ext(web$audio)
web$match <- web$sound == web$Resp
table(web$match)
# add trial
web$trial <- NA
for (subject in unique(web$ID)) {
idx <- which(web$ID == subject)
web$trial[idx] <- 1:length(idx)
}
table(web$trial)
out <- web[, c('ID', 'sex', 'Language', 'Name', 'Script', 'Family', 'Autotyp_Area', 'trial', 'sound', 'match')]
rm(idx, subject)
# Rename column names for consistency with the main analysis:
out <- rename(out, Condition = sound)
write_csv(out, paste0(data,'web_raw_trials.csv'))
# Chunk 40: ID tabs
ID_tabs <- with(web, table(Condition, Resp, ID))
# Chunk 41: matches
matches <- numeric(dim(ID_tabs)[3])
for (i in seq_along(matches)) {
matches[i] <- as.integer(sum(diag(ID_tabs[, , i])) == 2)
}
# Chunk 42: ids
ids <- unique(web$ID)
r_ppt <- tibble(ID = ids, Match = matches)
rm(ids, i, ID_tabs, matches)
# Chunk 43: get order for r_ppt
web <- web %>%
group_by(ID) %>%
mutate(Order = ifelse(trial == 1 & n() == 2,
ifelse(sound == "l", "l_first", "r_first"),
NA)) %>%
fill(Order, .direction = "updown") %>%
ungroup()
# Chunk 44: merge ppt
r_ppt$Language <- web[match(r_ppt$ID, web$ID), ]$Language
r_ppt$Sex <- web[match(r_ppt$ID, web$ID), ]$sex
r_ppt$Age <- web[match(r_ppt$ID, web$ID), ]$participantage
r_ppt$Name <- web[match(r_ppt$ID, web$ID), ]$Name
r_ppt$Script <- web[match(r_ppt$ID, web$ID), ]$Script
r_ppt$Family <- web[match(r_ppt$ID, web$ID), ]$Family
r_ppt$Autotyp_Area <- web[match(r_ppt$ID, web$ID), ]$Autotyp_Area
r_ppt$L2 <- web[match(r_ppt$ID, web$ID), ]$L2_raw
r_ppt$EnglishL2YesNo <- web[match(r_ppt$ID, web$ID), ]$EnglishL2YesNo
r_ppt$Order <- web[match(r_ppt$ID, web$ID), ]$Order
r_ppt$r_l_distinction_L1 <- web[match(r_ppt$ID, web$ID), ]$r_l_distinction_L1
r_ppt$trill_real_L1 <- web[match(r_ppt$ID, web$ID), ]$trill_real_L1
r_ppt$trill_occ_L1 <- web[match(r_ppt$ID, web$ID), ]$trill_occ_L1
r_ppt$r_l_distinction_L2 <- web[match(r_ppt$ID, web$ID), ]$r_l_distinction_L2
r_ppt$trill_real_L2 <- web[match(r_ppt$ID, web$ID), ]$trill_real_L2
r_ppt$trill_occ_L2 <- web[match(r_ppt$ID, web$ID), ]$trill_occ_L2
# Chunk 45: save cleaned data
write_csv(r_ppt, paste0(data, 'web_experiment_cleaned.csv'))
web <- read_csv(paste0(data, 'web_experiment_cleaned.csv'))
# Chunk 46: remove objects web
rm(L2_info, langs, out, ppt_N, r_ppt)
# Chunk 47: load demographics field
# Load:
demographics <- read_delim(paste0(data, 'fieldwork-personal-data.csv'), delim = ';')
# Chunk 48: fix colname
colnames(field)[ncol(field)] <- 'PA08'
# Chunk 49: reshape table
# Remove rows with names "bouba-ort", "bouba-aud", "kiki-ort", and "kiki-aud"
field <- field %>%
filter(!file %in% c("bouba-ort", "bouba-aud", "kiki-ort", "kiki-aud"))
field[1, -1] <- gsub("rock", "r", field[1, -1])
field <- field %>%
select(-file) %>%
pivot_longer(everything(), names_to = "ID", values_to = "value") %>%
arrange(ID) %>%  # Arrange by ID
group_by(ID) %>%
mutate(audio = rep(c("r", "l"), each = n() / 2),  # Repeat "r" and "l" for each ID
response = ifelse(value %in% c("0", "NA", " "), NA, value)) %>%
ungroup() %>%
select(-value)  # Remove the original "value" column
# Chunk 50: get lang info
field <- mutate(field,
Language = str_extract(ID, '[A-Z]+'),
Participant = str_extract(ID, '\\d+'))
# Chunk 51: add age gender
# Rename so that key columns match names:
demographics <- rename(demographics,
ID = subject,
Age = age,
Sex = gender,
L2 = "other-lang")
# Get only that what is needed:
demographics <- select(demographics,
ID, Age, Sex, L2)
# Join:
field <- left_join(field, demographics)
# Chunk 52: exclude field
field <- filter(field, Age > 18)
# Chunk 53: exclude polish
field <- filter(field, Language != 'US')
field <- filter(field, Language != 'PL')
# Chunk 54
field %>%
group_by(ID) %>%
summarise(AllResponsesEmptyOrNA = all(is.na(response) | response == "")) %>%
filter(AllResponsesEmptyOrNA)
# Chunk 55
field <- field %>%
filter(ID != "BE40")
# Chunk 56: add info on contrast
field$Name <- NA
field[field$Language == 'BE', ]$Name <- 'english'
field[field$Language == 'DE', ]$Name <- 'german'
field[field$Language == 'SR', ]$Name <- 'portuguese'
field[field$Language == 'VA', ]$Name <- 'daakie'
field[field$Language == 'BR', ]$Name <- 'berber'
field[field$Language == 'PA', ]$Name <- 'palikur'
# Chunk 57: process l2
L2_info_field <- filter(field, !duplicated(ID)) %>% select(ID, Language, Sex, Name, L2)
# Chunk 58: add no l2
L2_info_field <- L2_info_field %>%
mutate(L2 = ifelse(L2 %in% c('0', 'NA', ' '), NA, L2)) %>%
mutate(L2 = ifelse(is.na(L2), 'no_L2', L2))
# Chunk 59: add english
L2_info_field <- mutate(L2_info_field,
EnglishL2YesNo = ifelse(grepl('english', L2, ignore.case = TRUE), TRUE, FALSE))
# Chunk 60: l2 conversions
L2_info_field <- L2_info_field %>%
mutate(L2 = str_replace_all(L2, regex('British Sign Language', ignore_case = TRUE), 'BSL'),
L2 = str_replace_all(L2, regex('Mandarin', ignore_case = TRUE), 'chinese'),
L2 = str_replace_all(L2, regex('Moroccan Arabic', ignore_case = TRUE), 'arabic'),
L2 = str_replace_all(L2, regex('Standard Arabic', ignore_case = TRUE), 'arabic'))
L2_info_field <- L2_info_field %>%
mutate(L2 = ifelse(L2 %in% c("no_L2", "BSL"), L2, str_to_lower(L2))) %>%
rowwise() %>%
mutate(L2 = paste(unique(unlist(str_split(L2, ",\\s*"))), collapse = ", "))
L2_info_field$L2 <- sub("\\s*\\(restricted reading skills\\)", "", L2_info_field$L2)
# Chunk 61: add rhotic cols field
L2_info_field <- L2_info_field %>%
mutate(r_l_distinction_L1 = NA,
trill_real_L1 = NA,
trill_occ_L1 = NA,
r_l_distinction_L2 = NA,
trill_real_L2 = NA,
trill_occ_L2 = NA)
# Chunk 62: add rhotic info l1 field
L2_info_field <- L2_info_field %>%
left_join(languages_data, by = c("Name" = "Languages")) %>%
mutate(r_l_distinction_L1 = coalesce(r_l_distinction, r_l_distinction_L1),
trill_real_L1 = coalesce(trill_real, trill_real_L1),
trill_occ_L1 = coalesce(trill_occ, trill_occ_L1)) %>%
select(-c(glottocode, r_l_distinction, trill_real, trill_occ))
# Chunk 63: add rhotic info l2 field
# Save the original 'l2' column
original_L2 <- L2_info_field$L2
# Run the main code
L2_info_field <- L2_info_field %>%
separate_rows(L2, sep = ",\\s*") %>%
left_join(languages_data, by = c("L2" = "Languages")) %>%
group_by(ID) %>%
mutate(r_l_distinction_L2 = if(all(is.na(r_l_distinction))) NA else max(coalesce(r_l_distinction, r_l_distinction_L2), na.rm = TRUE),
trill_real_L2 = if(all(is.na(trill_real))) NA else max(coalesce(trill_real, trill_real_L2), na.rm = TRUE),
trill_occ_L2 = if(all(is.na(trill_occ))) NA else max(coalesce(trill_occ, trill_occ_L2), na.rm = TRUE)) %>%
select(-c(glottocode, r_l_distinction, trill_real, trill_occ)) %>%
distinct(ID, .keep_all = TRUE) %>%
ungroup()
# Replace the original 'L2' column
L2_info_field$L2 <- original_L2
rm(original_L2)
# Chunk 64: l2 and field
field <- left_join(field, select(L2_info_field, -Language), by = c('ID' = 'ID'))
# Chunk 65: rename vars field
field <- field %>%
rename(Name = Name.x,
L2_raw = L2.x,
L2_cleaned = L2.y) %>%
select(-Name.y)
# Chunk 66: how many per participant field
ppt_N <- field %>% count(ID)
# Chunk 67: check response N field
all(ppt_N$n == 2)
# Chunk 68: remove less than 2 field
rm(ppt_N)
# Chunk 69: counts per lang field
field %>% count(Name) %>%
mutate(n = n / 2) %>%
print()
# Chunk 70: load lang file field
langs <- read_csv(paste0(data, 'language_info_field.csv'))
# Chunk 71: merge lang field
field <- left_join(field, langs, by = "Language")
field <- field %>%
rename(Name = Name.y) %>%
select(-Name.x) %>%
rename(Sex = Sex.y) %>%
select(-Sex.x)
# Chunk 72: congruency field
field$match = field$audio == field$response
# Chunk 73: matches field
field <- field %>%
mutate(match = ifelse(is.na(match), FALSE, match))
# Chunk 74: create output field
out <- field[, c('ID', 'Sex', 'Language', 'Name', 'Script', 'Family', 'Autotyp_Area', 'audio', 'match')]
# Rename column names for consistency with the main analysis:
out <- rename(out, Condition = audio)
write_csv(out, paste0(data,'field_raw_trials.csv'))
# Chunk 75: ids field
r_ppt <- field %>%
group_by(ID) %>%
summarise(Match = as.integer(all(match == TRUE)))
# Chunk 76: merge ppt field
r_ppt$Language <- field[match(r_ppt$ID, field$ID), ]$Language
r_ppt$Sex <- field[match(r_ppt$ID, field$ID), ]$Sex
r_ppt$Age <- field[match(r_ppt$ID, field$ID), ]$Age
r_ppt$Name <- field[match(r_ppt$ID, field$ID), ]$Name
r_ppt$Script <- field[match(r_ppt$ID, field$ID), ]$Script
r_ppt$Family <- field[match(r_ppt$ID, field$ID), ]$Family
r_ppt$Autotyp_Area <- field[match(r_ppt$ID, field$ID), ]$Autotyp_Area
r_ppt$L2 <- field[match(r_ppt$ID, field$ID), ]$L2_raw
r_ppt$EnglishL2YesNo <- field[match(r_ppt$ID, field$ID), ]$EnglishL2YesNo
r_ppt$r_l_distinction_L1 <- field[match(r_ppt$ID, field$ID), ]$r_l_distinction_L1
r_ppt$trill_real_L1 <- field[match(r_ppt$ID, field$ID), ]$trill_real_L1
r_ppt$trill_occ_L1 <- field[match(r_ppt$ID, field$ID), ]$trill_occ_L1
r_ppt$r_l_distinction_L2 <- field[match(r_ppt$ID, field$ID), ]$r_l_distinction_L2
r_ppt$trill_real_L2 <- field[match(r_ppt$ID, field$ID), ]$trill_real_L2
r_ppt$trill_occ_L2 <- field[match(r_ppt$ID, field$ID), ]$trill_occ_L2
# Chunk 77: save cleaned data field
write_csv(r_ppt, paste0(data, 'field_experiment_cleaned.csv'))
field <- read_csv(paste0(data, 'field_experiment_cleaned.csv'))
View(field)
rm(L2_info_field, langs, out, ppt_N, r_ppt, demographics)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse) # data processing
library(brms) # bayesian models
library(cmdstanr)
library(ggdist) # for plotting
# option for Bayesian regression models:
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
# Set the script's path as working directory
parentfolder = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(parentfolder))
parentfolder <- getwd()
models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
data          <- paste0(parentfolder, '/data/')
# Chunk 3
packageVersion('tidyverse')
packageVersion('ggplot2')
packageVersion('brms')
packageVersion('cmdstanr')
packageVersion('ggdist')
# Chunk 4
source('theme_timo.R')
# Chunk 5
web <- read_csv(paste0(data, 'web_experiment_cleaned.csv'))
web_raw <- read_csv(paste0(data, '/web_raw_trials.csv'))
field <- read_csv(paste0(data, 'field_experiment_cleaned.csv'))
field_raw <- read_csv(paste0(data, '/field_raw_trials.csv'))
# Chunk 6
nrow(web)
# Chunk 7
table(web$sex)
table(web$Sex)
summary(web$Age)
web %>% count(Family) %>% nrow()
nrow(field)
table(field$Sex)
nrow(field)
table(field$Sex)
summary(field$Age)
field %>% count(Language) %>% nrow()
field %>% count(Family) %>% nrow()
field <- read.csv(paste0(data, "fieldwork-responses-rest.csv"), sep = ";")
web <- read.csv(paste0(data, "r_l_web.csv"), sep = ",")
languages_data <- read.csv(paste0(data, "languages_data.csv"), sep = ",")
web <- mutate(web,
Language = str_extract(experiment, '[A-Z]{2,2}'))
web <- mutate(web,
ID = str_c(Language, '_', session))
web <- filter(web,
l1 != 'q')
web <- filter(web,
l1 != 'b')
web <- filter(web,
firstlanguage != 'q')
length(unique(web$ID))
975-903
903*100/975
100-(903*100/975)
# Load:
demographics <- read_delim(paste0(data, 'fieldwork-personal-data.csv'), delim = ';')
colnames(field)[ncol(field)] <- 'PA08'
# Remove rows with names "bouba-ort", "bouba-aud", "kiki-ort", and "kiki-aud"
field <- field %>%
filter(!file %in% c("bouba-ort", "bouba-aud", "kiki-ort", "kiki-aud"))
field[1, -1] <- gsub("rock", "r", field[1, -1])
field <- field %>%
select(-file) %>%
pivot_longer(everything(), names_to = "ID", values_to = "value") %>%
arrange(ID) %>%  # Arrange by ID
group_by(ID) %>%
mutate(audio = rep(c("r", "l"), each = n() / 2),  # Repeat "r" and "l" for each ID
response = ifelse(value %in% c("0", "NA", " "), NA, value)) %>%
ungroup() %>%
select(-value)  # Remove the original "value" column
field <- mutate(field,
Language = str_extract(ID, '[A-Z]+'),
Participant = str_extract(ID, '\\d+'))
# Rename so that key columns match names:
demographics <- rename(demographics,
ID = subject,
Age = age,
Sex = gender,
L2 = "other-lang")
# Get only that what is needed:
demographics <- select(demographics,
ID, Age, Sex, L2)
# Join:
field <- left_join(field, demographics)
View(field)
length(unique(field$ID))
126*100/159
100-(126*100/159)
field <- filter(field, Language != 'US')
field <- filter(field, Language != 'PL')
length(unique(field$ID))
133-126
100-(126*100/133)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse) # data processing
library(brms) # bayesian models
library(cmdstanr)
library(ggdist) # for plotting
# option for Bayesian regression models:
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
# Set the script's path as working directory
parentfolder = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(parentfolder))
parentfolder <- getwd()
models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
data          <- paste0(parentfolder, '/data/')
# Chunk 3
packageVersion('tidyverse')
packageVersion('ggplot2')
packageVersion('brms')
packageVersion('cmdstanr')
packageVersion('ggdist')
# Chunk 4
source('theme_timo.R')
# Chunk 5
web <- read_csv(paste0(data, 'web_experiment_cleaned.csv'))
web_raw <- read_csv(paste0(data, '/web_raw_trials.csv'))
field <- read_csv(paste0(data, 'field_experiment_cleaned.csv'))
field_raw <- read_csv(paste0(data, '/field_raw_trials.csv'))
# Chunk 6
nrow(web)
# Chunk 7
table(web$Sex)
table(web$r_first)
View(web)
table(web$Order)
count(table(web$Order))
prop.table(table(web$Order)) * 100
nrow(field)
table(field$Sex)
summary(field$Age)
field %>% count(Language) %>% nrow()
field %>% count(Name) %>% nrow()
field %>% count(Family) %>% nrow()
field %>% count(Name, r_l_distinction_L1) %>% count(r_l_distinction_L1)
field %>% count(Name, trill_real_L1) %>% count(trill_real_L1)
field %>% count(Name, trill_occ_L1) %>% count(trill_occ_L1)
field %>% count(Name, r_l_distinction_L2) %>% count(r_l_distinction_L2)
field %>% count(Name, trill_real_L2) %>% count(trill_real_L2)
field %>% count(Name, trill_occ_L2) %>% count(trill_occ_L2)
mean(field$Match)
field %>%
filter(r_l_distinction_L1 == "0") %>%
summarize(mean_match = mean(Match, na.rm = TRUE))
field %>%
filter(r_l_distinction_L1 == "0") %>%
filter(!EnglishL2YesNo) %>%
filter(r_l_distinction_L1 == '0') %>%
summarize(mean_match = mean(Match, na.rm = TRUE))
field_avg <- field %>%
group_by(Name) %>%
summarize(M = mean(Match)) %>%
arrange(desc(M)) %>%
mutate(percent = round(M, 2) * 100,
percent = str_c(percent, '%'))
# Show:
field_avg %>% print(n = Inf)
field %>%
count(Name, sort = TRUE) %>% print(n = Inf)
field %>% count(r_l_distinction_L1) %>%
mutate(prop = n / sum(n),
percent = round(prop, 2) * 100,
percent = str_c(percent, '%'))
sum(is.na(field$L2)) / nrow(field)
# bilinguals:
1 - sum(is.na(field$L2)) / nrow(field)
field %>% count(EnglishL2YesNo) %>%
mutate(prop = n / sum(n),
percent = round(prop, 2) * 100,
percent = str_c(percent, '%'))
field %>%
filter(r_l_distinction_L1 == '0') %>%
count(r_l_distinction_L2 == '1') %>%
mutate(prop = n / sum(n),
percent = round(prop, 2) * 100,
percent = str_c(percent, '%'))
field %>%
filter(r_l_distinction_L1 == '0') %>%  # excludes English as well
filter(!EnglishL2YesNo) %>%
filter(r_l_distinction_L1 == '0') %>%
nrow()
field %>%
filter(r_l_distinction_L1 == '0') %>%
count(r_l_distinction_L2 == '1') %>%
mutate(prop = n / sum(n),
percent = round(prop, 2) * 100,
percent = str_c(percent, '%'))
field %>%
filter(r_l_distinction_L1 == '0') %>%  # excludes English as well
filter(!EnglishL2YesNo) %>%
filter(r_l_distinction_L1 == '0') %>%
nrow()
field %>%
filter(r_l_distinction_L1 == '0' & !EnglishL2YesNo & r_l_distinction_L1 == '0') %>%
print(n = 50)
field %>%
filter(r_l_distinction_L1 == '0') %>%  # excludes English as well
filter(!EnglishL2YesNo) %>%
filter(r_l_distinction_L1 == '0') %>%
count(Language)
field %>%
filter(r_l_distinction_L1 == '0') %>%  # excludes English as well
filter(!EnglishL2YesNo) %>%
filter(r_l_distinction_L1 == '0') %>%
count(Match) %>%
mutate(prop = n / sum(n),
percent = round(prop, 2) * 100,
percent = str_c(percent, '%'))
unlink("r_l_modeling_v2_cache", recursive = TRUE)
unlink("r_l_modeling_v2_cache", recursive = TRUE)
unlink("r_l_modeling_v2_cache", recursive = TRUE)
