---
title: "R/L across languages: Modeling"
author: "Aleksandra Ćwiek & Dan Dediu"
date: "2023-10-26"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float: yes
    df_print: paged
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_notebook:
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction:

This script uses the output of "r_l_preparation.Rmd" and returns all
values reported in the paper.

# Setup:

Load packages:

```{r, message = FALSE, warning = FALSE}
library(tidyverse) # data processing
library(brms) # bayesian models
#library(cmdstanr) # install it with: install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos"))) followed by install_cmdstan()
library(ggdist) # for plotting
# option for Bayesian regression models:
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())

## Set the script's path as working directory
#parentfolder = rstudioapi::getActiveDocumentContext()$path 
#setwd(dirname(parentfolder))
#parentfolder <- getwd()
parentfolder <- "."; # assume current folder is the document folder

models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
data          <- paste0(parentfolder, '/data/')
```

For reproducible reporting:

```{r, message = FALSE, warning = FALSE}
packageVersion('tidyverse')
packageVersion('ggplot2')
packageVersion('brms')
#packageVersion('cmdstanr')
packageVersion('ggdist')
```

Load ggplot2 theme and colors:

```{r, message = FALSE, warning = FALSE}
source('theme_timo.R')

colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                       "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

Load data:

```{r, message = FALSE, warning = FALSE}
web     <- read_csv(paste0(data, 'web_experiment_cleaned.csv'))
web_raw <- read_csv(paste0(data, '/web_raw_trials.csv'))

field     <- read_csv(paste0(data, 'field_experiment_cleaned.csv'))
field_raw <- read_csv(paste0(data, '/field_raw_trials.csv'))
```

# Online experiment

## Descriptive statistics

First, how many participants?

```{r}
nrow(web)
```

Sex division

```{r}
table(web$Sex)
```

Ages

```{r}
summary(web$Age)
```

Order division

```{r}
# Counts
table(web$Order)
# Percentage
prop.table(table(web$Order)) * 100
```

First, how many languages?

```{r}
web %>% count(Name) %>% nrow()
```

Does this number correspond with the L1s?

```{r}
web %>% count(Language) %>% nrow()
```

How many families?

```{r}
web %>% count(Family) %>% nrow()
```

How many have the R/L distinction in the L1 among the languages?

```{r}
web %>% count(Language, r_l_distinction_L1) %>% count(r_l_distinction_L1)
```

How many really use the alveolar trill in L1 among the languages?

```{r}
web %>% count(Language, trill_real_L1) %>% count(trill_real_L1)
```

How many really have the alveolar trill in L1 as an allophone among the
languages?

```{r}
web %>% count(Language, trill_occ_L1) %>% count(trill_occ_L1)
```

What about the same questions for L2. But this will not neatly sum up to
25, due to various possible scenarios for L2 within a specific L1.

How many have the R/L distinction in the L2 among the languages?

```{r}
web %>% count(Language, r_l_distinction_L2) %>% count(r_l_distinction_L2)
```

How many really use the alveolar trill in L2 among the languages?

```{r}
web %>% count(Language, trill_real_L2) %>% count(trill_real_L2)
```

How many really have the alveolar trill in L2 as an allophone among the
languages?

```{r}
web %>% count(Language, trill_occ_L2) %>% count(trill_occ_L2)
```

What is the grand average congruent behavior?

```{r}
mean(web$Match)
```

87.3%

What about only among those who have L1 without the distinction?

```{r}
web %>%
  filter(r_l_distinction_L1 == "0") %>%
  summarize(mean_match = mean(Match, na.rm = TRUE))
```

83.9%

What about only among those who have L1 without the distinction and no
L2 that distinguishes?

```{r}
web %>%
  filter(r_l_distinction_L1 == "0") %>%
  filter(!EnglishL2YesNo) %>% 
  filter(r_l_distinction_L1 == '0') %>% 
  summarize(mean_match = mean(Match, na.rm = TRUE))
```

85.1%

Compute average matching behavior per language and sort:

```{r}
web_avg <- web %>%
  group_by(Language) %>% 
  summarize(M = mean(Match)) %>% 
  arrange(desc(M)) %>% 
  mutate(percent = round(M, 2) * 100,
         percent = str_c(percent, '%'))

# Show:

web_avg %>% print(n = Inf)
```

Check some demographics, also to report in the paper. First, the number
of participants per language:

```{r}
web %>% 
  count(Name, sort = TRUE) %>% print(n = Inf)
```

Then, the number of L1 speakers who have R/L distinction vs. who don't:

```{r}
web %>% count(r_l_distinction_L1) %>%
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

How many people do not have any L2?

```{r}
sum(is.na(web$L2)) / nrow(web)

# bilinguals:
1 - sum(is.na(web$L2)) / nrow(web)
```

Check how many people knew English as their L2:

```{r}
web %>% count(EnglishL2YesNo) %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

Of those that don't use a R/L distinction in their L1, how many use R/L
distinction in their L2? (double-check if logic alright!)

```{r}
web %>%
  filter(r_l_distinction_L1 == 0) %>% 
  count(r_l_distinction_L2 == 1) %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

How many "pure" speakers were there?, i.e., those people that 1) don't
know English, 2) don't use an L1 with a R/L distinction, and 3) don't
know an L2 that distinguishes R/L.

```{r}
web %>% 
  filter(r_l_distinction_L1 == 0) %>%  # excludes English as well
  filter(!EnglishL2YesNo) %>% 
  filter(r_l_distinction_L2 == 0) %>% 
  nrow()
```

1 person!

Let's check if this is correct. This gives the list of all participants
for whom this applies.

```{r}
web %>% 
    filter(r_l_distinction_L1 == 0 & !EnglishL2YesNo & r_l_distinction_L2 == 0) %>% 
    print(n = 50)
```

Are these really "pure"? What languages do they speak?

```{r}
web %>% 
  filter(r_l_distinction_L1 == 0) %>%  # excludes English as well
  filter(!EnglishL2YesNo) %>% 
  filter(r_l_distinction_L2 == 0) %>% 
  count(Language)
```

One Japanese speaker.

Nevertheless, let's explore whether these also show matches?

```{r}
web %>% 
  filter(r_l_distinction_L1 == 0) %>%  # excludes English as well
  filter(!EnglishL2YesNo) %>% 
  filter(r_l_distinction_L2 == 0) %>% 
  count(Match) %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

Yes, similar to above 85%.


## Regression models

Set parameters that will be used for all MCMC sampling:

```{r}
mywarmup <- 4000
myiter   <- 8000
```

Check the distribution of scripts across families to make decisions
about random effects structure:

```{r}
table(web$Family, web$r_l_distinction_L1)
```

Let's make Order a factor with 'r_first' baseline:

```{r}
web <- mutate(web,
               Order = factor(Order, levels = c('r_first', 'l_first')))
```

Check that the order effect is approximately balanced:

```{r}
web %>% count(Order) %>%
  mutate(prop = n / sum(n))
```

<!-- @Dan & @Bodo, can you please check if we need some manipulation, like
contrast-coding but weighted? Bodo, I know for bouba/kiki you did some
kind of weighted modification and I'm sure that the proportions of our
predictors are not balanced, see below:

Dan: no idea what this means, so I'll use them as they are (aka binary factors)...
-->

```{r}
web %>% count(r_l_distinction_L1) %>%
  mutate(prop = n / sum(n))
# highly imbalanced: 15.8% vs 84.2%

web %>% count(trill_real_L1) %>%
  mutate(prop = n / sum(n))
# ok: 58.8% vs 41.2%

web %>% count(trill_occ_L1) %>%
  mutate(prop = n / sum(n))
# 100% are 1 --> excluded from the model

## And for L2, just in case
web %>% count(r_l_distinction_L2) %>%
  mutate(prop = n / sum(n))
# 13.5% missing, the rest almost all (86.3%) are 1 and 0.2% are 0 ---> exclude as well

web %>% count(trill_real_L2) %>%
  mutate(prop = n / sum(n))
# 13.5% missing, the rest is balanced: 53.8% vs 32.7%

web %>% count(trill_occ_L2) %>%
  mutate(prop = n / sum(n))
# 13.5% missing, the rest are all (86.5%) are 1 ---> exclude as well
```

For now, I will just use them as they are coded.

```{r}
web <- mutate(web,
               order_num = ifelse(Order == 'r_first', -0.5, +0.5))

# Code them as factors:
web$r_l_distinction_L1_f <- factor(c("same", "distinct")[web$r_l_distinction_L1 + 1], levels=c("same", "distinct"));
web$trill_real_L1_f      <- factor(c("no", "yes")[web$trill_real_L1 + 1], levels=c("no", "yes"));
web$trill_occ_L1_f       <- factor(c("no", "yes")[web$trill_occ_L1 + 1], levels=c("no", "yes"));
web$r_l_distinction_L2_f <- factor(c("same", "distinct")[web$r_l_distinction_L2 + 1], levels=c("same", "distinct"));
web$trill_real_L2_f      <- factor(c("no", "yes")[web$trill_real_L2 + 1], levels=c("no", "yes"));
web$trill_occ_L2_f       <- factor(c("no", "yes")[web$trill_occ_L2 + 1], levels=c("no", "yes"));
```


```{r L1 (take 2)}
# Various auxiliary functions:
library(parallel);
library(lme4);
library(performance);
library(brms);
library(bayestestR);
library(ggplot2);
library(gridExtra);
library(ggpubr);
library(sjPlot);

brms_ncores  <- max(detectCores(all.tests=TRUE, logical=FALSE), 4, na.rm=TRUE); # try to use multiple cores, if present

# Verbal interpretation of Bayes factors (BF):
BF_interpretation <- function(BF, model1_name="m1", model2_name="m2")
{
  if( BF > 100 )   return (paste0("extreme evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 30 )    return (paste0("very strong evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 10 )    return (paste0("strong evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 3 )     return (paste0("moderate evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 1 )     return (paste0("anecdotal evidence for ",model1_name," against ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF == 1 )    return (paste0("no evidence for ",model1_name," nor ",model2_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.33 )  return (paste0("anecdotal evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.10 )  return (paste0("moderate evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.033 ) return (paste0("strong evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  if( BF > 0.010 ) return (paste0("very strong evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
  return (paste0("extreme evidence for ",model2_name," against ",model1_name, " (BF=",sprintf("%.3g",BF),")"));
}

# Here I hack brms' kfold code to make it run in parallel using good old mclapply instead of futures
# this avoid random crashes which seem to be due to future, but works only on *NIX (which, for me here, is not an issue)
# Adapted the code from https://github.com/paul-buerkner/brms/blob/master/R/loo.R and https://github.com/paul-buerkner/brms/blob/master/R/kfold.R
if( Sys.info()['sysname'] == "Windows" )
{
  # In Windows, fall back to the stadard implementation in brms:
  add_criterion_kfold_parallel <- function(model, K=10, chains=1)
  {
    return (add_criterion(model, criterion="kfold", K=K, chains=chains));
  }
} else
{
  # On anything else, try to use maclapply:
  add_criterion_kfold_parallel <- function(model, K=10, chains=1)
  {
    model <- restructure(model);
  
    mf <- model.frame(model); 
    attributes(mf)[c("terms", "brmsframe")] <- NULL;
    N <- nrow(mf);
    
    if( K > N ) return (model); # does not work in this case...
    
    fold_type <- "random"; folds <- loo::kfold_split_random(K, N);
    Ksub <- seq_len(K);
  
    kfold_results <- mclapply(Ksub, function(k) 
    {
      omitted <- predicted <- which(folds == k);
      mf_omitted <- mf[-omitted, , drop=FALSE];
      
      if( exists("subset_data2", envir=asNamespace("brms")) )
      {
        # Newer versions of brms:
        model_updated <- base::suppressWarnings(update(model, newdata=mf_omitted, data2=brms:::subset_data2(model$data2, -omitted), refresh=0, chains=chains));
        
        lppds <- log_lik(model_updated, newdata=mf[predicted, , drop=FALSE], newdata2=brms:::subset_data2(model$data2, predicted), 
                         allow_new_levels=TRUE, resp=NULL, combine=TRUE, chains=chains);
      } else if( exists("subset_autocor", envir=asNamespace("brms")) )
      {
        # Older versions of brms:
        model2 <- brms:::subset_autocor(model, -omitted, incl_car=TRUE);
        model_updated <- base::suppressWarnings(update(model2, newdata=mf_omitted, refresh=0, chains=chains));
        
        lppds <- log_lik(model_updated, newdata=mf[predicted, , drop=FALSE], allow_new_levels=TRUE, resp=NULL, combine=TRUE, chains=chains);
      } else
      {
        stop("Unknown version of brms!");
      }
  
      return (list("obs_order"=predicted, "lppds"=lppds));
    }, mc.cores=ifelse(exists("brms_ncores"), brms_ncores, detectCores()));
    
    # Put them back in the form expected by the the following unmodifed code:
    obs_order <- lapply(kfold_results, function(x) x$obs_order);
    lppds     <- lapply(kfold_results, function(x) x$lppds);
    
    elpds <- brms:::ulapply(lppds, function(x) apply(x, 2, brms:::log_mean_exp))
    # make sure elpds are put back in the right order
    elpds <- elpds[order(unlist(obs_order))]
    elpd_kfold <- sum(elpds)
    se_elpd_kfold <- sqrt(length(elpds) * var(elpds))
    rnames <- c("elpd_kfold", "p_kfold", "kfoldic")
    cnames <- c("Estimate", "SE")
    estimates <- matrix(nrow = 3, ncol = 2, dimnames = list(rnames, cnames))
    estimates[1, ] <- c(elpd_kfold, se_elpd_kfold)
    estimates[3, ] <- c(-2 * elpd_kfold, 2 * se_elpd_kfold)
    out <- brms:::nlist(estimates, pointwise = cbind(elpd_kfold = elpds))
    atts <- brms:::nlist(K, Ksub, NULL, folds, fold_type)
    attributes(out)[names(atts)] <- atts
    out <- structure(out, class = c("kfold", "loo"))
    
    attr(out, "yhash") <- brms:::hash_response(model, newdata=NULL, resp=NULL);
    attr(out, "model_name") <- "";
    
    model$criteria$kfold <- out;
    model;
  }
}

# Bayesian fit indices for a given model:
brms_fit_indices <- function(model, indices=c("bayes_R2", "loo", "waic", "kfold"), K=10, verbose=TRUE, do.parallel=TRUE)
{
  if( "bayes_R2" %in% indices )
  {
    if( verbose) cat("R^2...\n");
    #attr(model, "R2") <- bayes_R2(model); 
    model <- add_criterion(model, "bayes_R2"); 
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "bayes_R2" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "bayes_R2") ]] <- NULL;
  }
  
  if( "loo" %in% indices )
  {
    if( verbose) cat("LOO...\n");
    model <- add_criterion(model, "loo"); 
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "loo" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "loo") ]] <- NULL;
  }
  
  if( "waic" %in% indices )
  {
    if( verbose) cat("WAIC...\n");
    model <- add_criterion(model, "waic"); 
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "waic" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "waic") ]] <- NULL;
  }
  
  if( "kfold" %in% indices )
  {
    if( verbose) cat(paste0("KFOLD (K=",K,")...\n"));
    model1 <- NULL;
    if( !do.parallel )
    {
      try(model1 <- add_criterion(model, "kfold", K=K, chains=1), silent=TRUE);
    } else
    {
      try(model1 <- add_criterion_kfold_parallel(model, K=K, chains=1), silent=TRUE);
    }
    if( !is.null(model1) )
    {
      model <- model1;
    } else
    {
      # Remove the criterion (if already there):
      if( !is.null(model$criteria) && "kfold" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "kfold") ]] <- NULL;
    }
  } else
  {
    # Remove the criterion (if already there):
    if( !is.null(model$criteria) && "kfold" %in% names(model$criteria) ) model$criteria[[ which(names(model$criteria) == "kfold") ]] <- NULL;
  }

  gc();
  
  return (model);
}

# Bayesian model comparison:
#model1 <- b_uvbm__blue
#model2 <- b_popsize__blue
brms_compare_models <- function(model1, model2, name1=NA, name2=NA, bayes_factor=TRUE, print_results=TRUE)
{
  if( !is.null(model1$criteria) && "bayes_R2" %in% names(model1$criteria) && !is.null(model1$criteria$bayes_R2) &&
      !is.null(model2$criteria) && "bayes_R2" %in% names(model2$criteria) && !is.null(model2$criteria$bayes_R2) )
  {
    R2_1_2 <- (mean(model1$criteria$bayes_R2) - mean(model2$criteria$bayes_R2));
  } else
  {
    R2_1_2 <- NA;
  }
  
  if( bayes_factor )
  {
    invisible(capture.output(bf_1_2 <- brms::bayes_factor(model1, model2)$bf));
    bf_interpret_1_2 <- BF_interpretation(bf_1_2, ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")); 
  }
  else
  {
    bf_1_2 <- NULL; bf_interpret_1_2 <- NA;
  }
  
  if( !is.null(model1$criteria) && "loo" %in% names(model1$criteria) && !is.null(model1$criteria$loo) &&
      !is.null(model2$criteria) && "loo" %in% names(model2$criteria) && !is.null(model2$criteria$loo) )
  {
    loo_1_2 <- loo_compare(model1, model2, criterion="loo", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
  } else
  {
    loo_1_2 <- NA;
  }
  
  if( !is.null(model1$criteria) && "waic" %in% names(model1$criteria) && !is.null(model1$criteria$waic) &&
      !is.null(model2$criteria) && "waic" %in% names(model2$criteria) && !is.null(model2$criteria$waic) )
  {
    waic_1_2 <- loo_compare(model1, model2, criterion="waic", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
    mw_1_2 <- model_weights(model1, model2, weights="waic", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
  } else
  {
    waic_1_2 <- NA; 
    mw_1_2 <- NA;
  }
  
  if( !is.null(model1$criteria) && "kfold" %in% names(model1$criteria) && !is.null(model1$criteria$kfold) &&
      !is.null(model2$criteria) && "kfold" %in% names(model2$criteria) && !is.null(model2$criteria$kfold) )
  {
    kfold_1_2 <- loo_compare(model1, model2, criterion="kfold", model_names=c(ifelse(!is.na(name1), name1, "model1"), ifelse(!is.na(name2), name2, "model2")));
  } else
  {
    kfold_1_2 <- NA;
  }
  
  if( print_results )
  {
    cat(paste0("\nComparing models '",ifelse(!is.na(name1), name1, "model1"),"' and '",ifelse(!is.na(name2), name2, "model2"),"':\n\n"));
    cat(paste0("\ndelta R^2 = ",sprintf("%.1f%%",100*R2_1_2),"\n\n"));
    cat(bf_interpret_1_2,"\n\n");
    cat("\nLOO:\n"); print(loo_1_2);
    cat("\nWAIC:\n"); print(waic_1_2);
    cat("\nKFOLD:\n"); print(kfold_1_2);
    cat("\nModel weights (WAIC):\n"); print(mw_1_2);
    cat("\n");
  }
  
  gc();
  
  return (list("R2"=R2_1_2, "BF"=bf_1_2, "BF_interpretation"=bf_interpret_1_2, "LOO"=loo_1_2, "WAIC"=waic_1_2, "KFOLD"=kfold_1_2, "model_weights_WAIC"=mw_1_2));
}

# print model comparisons
.print.model.comparison <- function(a=NULL, a.names=NULL, b=NULL) # a is the anova and b is the Bayesian comparison (only one can be non-NULL), a.names are the mappings between the inner and user-friendly model names
{
  # ANOVA:
  if( !is.null(a) )
  {
    a <- as.data.frame(a);
    if( !is.null(a.names) )
    {
      if( length(a.names) != nrow(a) || !all(names(a.names) %in% rownames(a)) )
      {
        stop("a.names do not correspond the anova model names!");
        return (NULL);
      }
      rownames(a) <- a.names[rownames(a)];
    }
    i <- which.min(a$AIC);
    s.a <- sprintf("%s %s %s: ΔAIC=%.1f, ΔBIC=%.1f", 
                   rownames(a)[i], 
                   ifelse((!is.na(a[2,"Pr(>Chisq)"]) && a[2,"Pr(>Chisq)"] <0.05) || (abs(a$AIC[1] - a$AIC[2]) > 3), ">", "≈"),
                   rownames(a)[3-i],
                   abs(a$AIC[1] - a$AIC[2]), 
                   abs(a$BIC[1] - a$BIC[2]));
    if( !is.na(a[2,"Pr(>Chisq)"]) )
    {
      s.a <- paste0(s.a,
                    sprintf(", *p*=%s", scinot(a[2,"Pr(>Chisq)"])));
    }
    
    # return value:
    return (s.a);
  }
  
  # Bayesian comparison:
  if( !is.null(b) )
  {
    s.b <- sprintf("BF: %s, ΔLOO(%s %s %s)=%.1f (%.1f), ΔWAIC(%s %s %s)=%.1f (%.1f), ΔKFOLD(%s %s %s)=%.1f (%.1f)",
                   # BF:
                   b$BF_interpretation, 
                   # LOO:
                   rownames(b$LOO)[1],
                   ifelse(abs(b$LOO[1,1]-b$LOO[2,1]) < 4 || abs(b$LOO[1,1]-b$LOO[2,1]) < b$LOO[2,2], "≈", ifelse(abs(b$LOO[1,1]-b$LOO[2,1]) < 2*b$LOO[2,2], ">", ">>")),
                   rownames(b$LOO)[2], 
                   abs(b$LOO[1,1]-b$LOO[2,1]), b$LOO[2,2], 
                   # WAIC:
                   rownames(b$WAIC)[1], 
                   ifelse(abs(b$WAIC[1,1]-b$WAIC[2,1]) < 4 || abs(b$WAIC[1,1]-b$WAIC[2,1]) < b$WAIC[2,2], "≈", ifelse(abs(b$WAIC[1,1]-b$WAIC[2,1]) < 2*b$WAIC[2,2], ">", ">>")), 
                   rownames(b$WAIC)[2], 
                   abs(b$WAIC[1,1]-b$WAIC[2,1]), b$WAIC[2,2],
                   # KFOLD:
                   rownames(b$KFOLD)[1], 
                   ifelse(abs(b$KFOLD[1,1]-b$KFOLD[2,1]) < 4 || abs(b$KFOLD[1,1]-b$KFOLD[2,1]) < b$KFOLD[2,2], "≈", ifelse(abs(b$KFOLD[1,1]-b$KFOLD[2,1]) < 2*b$KFOLD[2,2], ">", ">>")), 
                   rownames(b$KFOLD)[2], 
                   abs(b$KFOLD[1,1]-b$KFOLD[2,1]), b$KFOLD[2,2]);
    
    # return value:
    return (s.b);
  }
}

# Standard error of the mean:
std <- function(x) sd(x)/sqrt(length(x))

# Root Mean Square Error (RMSE) between observed (y) and predicted (yrep) values:
rmse <- function(y, yrep)
{
  yrep_mean <- colMeans(yrep)
  sqrt(mean((yrep_mean - y)^2))
}

# Log odds to probability (logistic regression):
lo2p <- function(x){ o <- exp(x); return (o/(1+o));}

# Scientific notation using Markdown conventions (inspired from https://www.r-bloggers.com/2015/03/scientific-notation-for-rlatex/):
scinot <- function(xs, digits=2, pvalue=TRUE)
{
  scinot1 <- function(x)
  {
    sign <- "";
    if(x < 0)
    {
      sign <- "-";
      x <- -x;
    }
    exponent <- floor(log10(x));
    if(exponent && pvalue && exponent < -3) 
    {
      xx <- round(x / 10^exponent, digits=digits);
      e <- paste0("×10^", round(exponent,0), "^");
    } else 
    {
      xx <- round(x, digits=digits+1);
      e <- "";
    }
    paste0(sign, xx, e);
  }
  vapply(xs, scinot1, character(1));
}

# Escape * in a string:
escstar <- function(s)
{
  gsub("*", "\\*", s, fixed=TRUE);
}




## Model fitting:

.save_models <- TRUE; # should we also save the actual models? these models are quite large on disk but allow later checks without re-running everything

if( !file.exists("./models/web_regressions_L1_summaries.rds") ||
    !(.save_models & file.exists("./models/web_regressions_L1_models.rds")) )
{
  # Actually fit the models and save various summaries (and potentially the actual models as well)
  # It's pretty computationally expensive, especially the Bayesian ones!
  
  # icc:
  m0 <- glmer(Match ~ 1 + 
                (1 | Language), # (1 | Family)  (1 | Autotyp_Area) have variance 0!
              data = web,
              family = binomial(), 
              control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m0); # looks good
  icc(m0); # 8.6%
  
  # check random slopes:
  m_Order <- glmer(Match ~ 1 + Order +
                     (1 | Language), # random slope for Order results in boundary (singular) fit
                   data = web,
                   family = binomial(), 
                   control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m_Order); # p=1.87e-05 ***
  anova(m_Order, m0); # p=1.266e-05 *** ΔAIC=-17.06
  
  m_Sex <- glmer(Match ~ 1 + Sex +
                   (1 | Language), # random slope for Sex results in boundary (singular) fit
                 data = web,
                 family = binomial(), 
                 control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m_Sex); # p=0.652
  anova(m_Sex, m0); # p=0.657 ΔAIC=1.8
  
  m_Age <- glmer(Match ~ 1 + Age +
                   (1 + Age | Language), # random slope for Age has tiny variance (0.0006003)
                 data = web,
                 family = binomial(), 
                 control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m_Age); # p=0.944
  anova(m_Age, m0); # p=0.26 ΔAIC=1.98
  
  m_rl <- glmer(Match ~ 1 + r_l_distinction_L1_f +
                  (1 + r_l_distinction_L1_f | Language), # decent variance (0.5186), all is good
                data = web,
                family = binomial(), 
                control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m_rl); # p=0.572
  anova(m_rl, m0); # p=0.93 ΔAIC=5.55
  
  m_trill <- glmer(Match ~ 1 + trill_real_L1_f +
                     (1 | Language), # random slope for trill_real_L1 has correlation slope-intercept 1.00 and boundary (singular) fit
                   data = web,
                   family = binomial(), 
                   control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m_trill); # p=0.201
  anova(m_trill, m0); # p=0.19 ΔAIC=0.31
  
  # VIF:
  m_vif <- glmer(Match ~ 1 + r_l_distinction_L1_f + trill_real_L1_f + Order + Sex + Age +
                   (1 | Language),
                 data = web,
                 family = binomial(), 
                 control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m_vif);
  performance::check_collinearity(m_vif); # all VIFs < 1.25, so OK
  
  # L2:
  m_rl2 <- glmer(Match ~ 1 + r_l_distinction_L2_f +
                   (1 + r_l_distinction_L2_f | Language), # decent variance (0.5123), all is good
                 data = web[ !is.na(web$r_l_distinction_L2_f), ],
                 family = binomial(), 
                 control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m_rl2); # p=0.814
  anova(m_rl2, update(m0, . ~ ., data=web[ !is.na(web$r_l_distinction_L2_f), ])); # p=0.96 ΔAIC=5.69
  
  m_trill2 <- glmer(Match ~ 1 + trill_real_L2_f +
                      (1 + trill_real_L2_f | Language),
                    data = web[ !is.na(web$trill_real_L2_f), ],
                    family = binomial(), 
                    control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m_trill2); # p=0.985
  anova(m_trill2, update(m0, . ~ ., data=web[ !is.na(web$trill_real_L2_f), ])); # p=0.76 ΔAIC=4.82
  
  # VIF:
  m_vif2 <- glmer(Match ~ 1 + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L2_f + trill_real_L2_f + Order + Sex + Age +
                    (1 | Language),
                  data = web,
                  family = binomial(), 
                  control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun = 1e7)));
  summary(m_vif2);
  performance::check_collinearity(m_vif2); # all VIFs < 1.25, so OK
  
  
  
  
  # with brms:
  
  # prior predictive checks:
  # what priors we can set (use Order for that):
  get_prior(Match ~ 1 + Order +
              (1 + Order | Language) +
              (1 + Order | Family) +
              (1 + Order | Autotyp_Area),
            data=web,
            family=bernoulli(link='logit'));
  # -> "sd" priors seem alright (student_t(3, 0, 2.5)), as does the "Intercept" (student_t(3, 0, 2.5)), but lkj(1) for "cor" might too accepting of extreme correlations, and (flat) for "b" is clearly not ok
  # so, we keep "sd" and "Intercept" but use lkj(2) for "cor" and student_t(5, 0, 2.5) for "b"
  b_priors <- brms::brm(Match ~ 1 + Order +
                          (1 + Order | Language) +
                          (1 + Order | Family) +
                          (1 + Order | Autotyp_Area),
                        data=web,
                        family=bernoulli(link='logit'),
                        prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                                brms::set_prior("lkj(2)", class="cor")),
                        sample_prior='only',  # needed for prior predictive checks
                        seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.999, max_treedepth=13));
  plot_prefix <- "./plots/web_L1";
  g <- arrangeGrob(pp_check(b_priors, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Prior predictive distribution of minimum values'),
                   pp_check(b_priors, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Prior predictive distribution of means'),
                   pp_check(b_priors, type='stat', stat='max') + xlab('p(match)') + ggtitle('Prior predictive distribution of maximum values'),
                   ncol=1); # seems fine but our value is a bit extreme
  as_ggplot(g); ggsave(paste0(plot_prefix,"_prior_predictive_checks.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  
  b <- b0 <- brm(Match ~ 1 + 
                   (1 | Language) +
                   (1 | Family) +
                   (1 | Autotyp_Area),
                 data = web,
                 family=bernoulli(link='logit'),
                 save_pars=save_pars(all=TRUE), # needed for Bayes factors
                 sample_prior=TRUE,  # needed for hypotheses tests
                 seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.999, max_treedepth=13));
  summary(b0); mcmc_plot(b0, type="trace"); mcmc_plot(b0); # very decent
  bayestestR::hdi(b0, ci=0.95);
  b0 <- brms_fit_indices(b0);
  # posterior predictive checks
  plot_prefix <- "./plots/web_L1_null"; b <- b0;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  
  b_Order <- brm(Match ~ 1 + Order +
                   (1 + Order | Language) +
                   (1 + Order | Family) +
                   (1 + Order | Autotyp_Area),
                 data = web,
                 family=bernoulli(link='logit'),
                 prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                         brms::set_prior("lkj(2)", class="cor")),
                 save_pars=save_pars(all=TRUE), # needed for Bayes factors
                 sample_prior=TRUE,             # needed for hypotheses tests
                 seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.999, max_treedepth=13));
  summary(b_Order); mcmc_plot(b_Order, type="trace"); mcmc_plot(b_Order); # very decent
  brms::hypothesis(b_Order, c("Orderl_first = 0")); # p=0.50
  b_Order <- brms_fit_indices(b_Order);
  brms_compare_models(b0, b_Order, "[null model]", "[+ order]"); # order clearly better
  # posterior predictive checks
  plot_prefix <- "./plots/web_L1_order"; b <- b_Order;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="Order"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  b_Sex <- brm(Match ~ 1 + Sex +
                 (1 + Sex | Language) +
                 (1 + Sex | Family) +
                 (1 + Sex | Autotyp_Area),
               data = web,
               family=bernoulli(link='logit'),
               prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                       brms::set_prior("lkj(2)", class="cor")),
               save_pars=save_pars(all=TRUE), # needed for Bayes factors
               sample_prior=TRUE,             # needed for hypotheses tests
               seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.999, max_treedepth=13));
  summary(b_Sex); mcmc_plot(b_Sex, type="trace"); mcmc_plot(b_Sex); # very decent
  brms::hypothesis(b_Sex, c("SexM = 0")); # p=0.84
  b_Sex <- brms_fit_indices(b_Sex);
  brms_compare_models(b0, b_Sex, "[null model]", "[+ sex]"); # sex does not matter
  # posterior predictive checks
  plot_prefix <- "./plots/web_L1_sex"; b <- b_Sex;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="Sex"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  b_Age <- brm(Match ~ 1 + Age +
                 (1 + Age | Language) +
                 (1 + Age | Family) +
                 (1 + Age | Autotyp_Area),
               data = web,
               family=bernoulli(link='logit'),
               prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                       brms::set_prior("lkj(2)", class="cor")),
               save_pars=save_pars(all=TRUE), # needed for Bayes factors
               sample_prior=TRUE,             # needed for hypotheses tests
               seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.999, max_treedepth=13));
  summary(b_Age); mcmc_plot(b_Age, type="trace"); mcmc_plot(b_Age); # very decent
  brms::hypothesis(b_Age, c("Age = 0")); # p=0.99
  b_Age <- brms_fit_indices(b_Age);
  brms_compare_models(b0, b_Age, "[null model]", "[+ age]"); # age and null seem equivalent
  # posterior predictive checks
  plot_prefix <- "./plots/web_L1_age"; b <- b_Age;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="Age"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  b_lr <- brm(Match ~ 1 + r_l_distinction_L1_f +
                (1 + r_l_distinction_L1_f | Language) +
                (1 + r_l_distinction_L1_f | Family) +
                (1 + r_l_distinction_L1_f | Autotyp_Area),
              data = web,
              family=bernoulli(link='logit'),
              prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                      brms::set_prior("lkj(2)", class="cor")),
              save_pars=save_pars(all=TRUE), # needed for Bayes factors
              sample_prior=TRUE,             # needed for hypotheses tests
              seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_lr); mcmc_plot(b_lr, type="trace"); mcmc_plot(b_lr); # very decent
  brms::hypothesis(b_lr, c("r_l_distinction_L1_fdistinct = 0")); # p=0.76
  b_lr <- brms_fit_indices(b_lr);
  brms_compare_models(b0, b_lr, "[null model]", "[+ l/r distinction]"); # l/r distinction is worse than null
  # posterior predictive checks
  plot_prefix <- "./plots/web_L1_lrdistinction"; b <- b_lr;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="r_l_distinction_L1_f"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  b_trill <- brm(Match ~ 1 + trill_real_L1_f +
                   (1 + trill_real_L1_f | Language) +
                   (1 + trill_real_L1_f | Family) +
                   (1 + trill_real_L1_f | Autotyp_Area),
                 data = web,
                 family=bernoulli(link='logit'),
                 prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                         brms::set_prior("lkj(2)", class="cor")),
                 save_pars=save_pars(all=TRUE), # needed for Bayes factors
                 sample_prior=TRUE,             # needed for hypotheses tests
                 seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_trill); mcmc_plot(b_trill, type="trace"); mcmc_plot(b_trill); # very decent
  brms::hypothesis(b_trill, c("trill_real_L1_fyes = 0")); # p=0.65
  b_trill <- brms_fit_indices(b_trill);
  brms_compare_models(b0, b_trill, "[null model]", "[+ trill]"); # trill might be better than null
  # posterior predictive checks
  plot_prefix <- "./plots/web_L1_trill"; b <- b_trill;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="trill_real_L1_f"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  # L2:
  b_lr2 <- brm(Match ~ 1 + r_l_distinction_L2_f +
                 (1 + r_l_distinction_L2_f | Language) +
                 (1 + r_l_distinction_L2_f | Family) +
                 (1 + r_l_distinction_L2_f | Autotyp_Area),
               data = web,
               family=bernoulli(link='logit'),
               prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                       brms::set_prior("lkj(2)", class="cor")),
               save_pars=save_pars(all=TRUE), # needed for Bayes factors
               sample_prior=TRUE,             # needed for hypotheses tests
               seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_lr2); mcmc_plot(b_lr2, type="trace"); mcmc_plot(b_lr2); # very decent
  brms::hypothesis(b_lr2, c("r_l_distinction_L2_fdistinct = 0")); # p=0.55
  #b_lr2 <- brms_fit_indices(b_lr2);
  #brms_compare_models(b0, b_lr2, "[null model]", "[+ l/r distinction in L2]"); # <-- needs refitting the null on the restricted data
  # posterior predictive checks
  plot_prefix <- "./plots/web_L2_lrdistinction"; b <- b_lr2;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="r_l_distinction_L2_f"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  b_trill2 <- brm(Match ~ 1 + trill_real_L2_f +
                    (1 + trill_real_L2_f | Language) +
                    (1 + trill_real_L2_f | Family) +
                    (1 + trill_real_L2_f | Autotyp_Area),
                  data = web,
                  family=bernoulli(link='logit'),
                  prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                          brms::set_prior("lkj(2)", class="cor")),
                  save_pars=save_pars(all=TRUE), # needed for Bayes factors
                  sample_prior=TRUE,             # needed for hypotheses tests
                  seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_trill2); mcmc_plot(b_trill2, type="trace"); mcmc_plot(b_trill2); # very decent
  brms::hypothesis(b_trill2, c("trill_real_L2_fyes = 0")); # p=0.77
  #b_trill2 <- brms_fit_indices(b_trill2);
  #brms_compare_models(b0, b_trill2, "[null model]", "[+ trill in L2]"); # trill might be better than null
  # posterior predictive checks
  plot_prefix <- "./plots/web_L2_trill"; b <- b_trill2;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="trill_real_L2_f"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  
  ## Complex models (with manual simplification):
  b_L1_full <- brm(Match ~ 1 + 
                     Order + # order (we know it has a main effect)
                     r_l_distinction_L1_f + trill_real_L1_f + # r/l (doesn't have a main effect) and trill (might have a main effect)
                     r_l_distinction_L1_f:Order + trill_real_L1_f:Order + # do they interact with order?
                     r_l_distinction_L1_f:trill_real_L1_f + # do they interact between them?
                     (1 + Order + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L1_f:Order + trill_real_L1_f:Order + r_l_distinction_L1_f:trill_real_L1_f | Language) + # full random effects structure (probably is an overkill)
                     (1 + Order + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L1_f:Order + trill_real_L1_f:Order + r_l_distinction_L1_f:trill_real_L1_f | Family) +
                     (1 + Order + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L1_f:Order + trill_real_L1_f:Order + r_l_distinction_L1_f:trill_real_L1_f | Autotyp_Area),
                   data = web,
                   family=bernoulli(link='logit'),
                   prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                           brms::set_prior("lkj(2)", class="cor")),
                   save_pars=save_pars(all=TRUE), # needed for Bayes factors
                   sample_prior=TRUE,             # needed for hypotheses tests
                   seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_L1_full); mcmc_plot(b_L1_full, type="trace", variable="^b_", regex=TRUE); mcmc_plot(b_L1_full, variable="^b_", regex=TRUE); # very decent
  brms::hypothesis(b_L1_full, c("Orderl_first = 0",                                      # p=0.43
                                "r_l_distinction_L1_fdistinct = 0",                      # p=0.72
                                "trill_real_L1_fyes = 0",                                # p=0.56
                                "Orderl_first:r_l_distinction_L1_fdistinct = 0",         # p=0.68
                                "Orderl_first:trill_real_L1_fyes = 0",                   # p=0.67
                                "r_l_distinction_L1_fdistinct:trill_real_L1_fyes = 0")); # p=0.55
  b_L1_full <- brms_fit_indices(b_L1_full);
  brms_compare_models(b0, b_L1_full, "[null model]", "[L1 full]"); # much better than null, but the questions is why...
  # posterior predictive checks
  plot_prefix <- "./plots/web_L1_full"; b <- b_L1_full;
  mcmc_plot(b, type="trace", variable="^b_", regex=TRUE); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b, variable="^b_", regex=TRUE); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms=c("trill_real_L1_f", "r_l_distinction_L1_f", "Order")); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  # manual simplification of the full model:
  # - the interactions of Order seem the least impressive, so start with them:
  b_L1_full_no_trill_order <- brm(Match ~ 1 + 
                                    Order + # order (we know it has a main effect)
                                    r_l_distinction_L1_f + trill_real_L1_f + # r/l (doesn't have a main effect) and trill (might have a main effect)
                                    r_l_distinction_L1_f:Order + # do they interact with order?
                                    r_l_distinction_L1_f:trill_real_L1_f + # do they interact between them?
                                    (1 + Order + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L1_f:Order + r_l_distinction_L1_f:trill_real_L1_f | Language) +
                                    (1 + Order + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L1_f:Order + r_l_distinction_L1_f:trill_real_L1_f | Family) +
                                    (1 + Order + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L1_f:Order + r_l_distinction_L1_f:trill_real_L1_f | Autotyp_Area),
                                  data = web,
                                  family=bernoulli(link='logit'),
                                  prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                                          brms::set_prior("lkj(2)", class="cor")),
                                  save_pars=save_pars(all=TRUE), # needed for Bayes factors
                                  sample_prior=TRUE,             # needed for hypotheses tests
                                  seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_L1_full_no_trill_order); mcmc_plot(b_L1_full_no_trill_order, type="trace", variable="^b_", regex=TRUE); mcmc_plot(b_L1_full_no_trill_order, variable="^b_", regex=TRUE); # very decent
  brms::hypothesis(b_L1_full_no_trill_order, c("Orderl_first = 0",                                      # p=0.42
                                               "r_l_distinction_L1_fdistinct = 0",                      # p=0.72
                                               "trill_real_L1_fyes = 0",                                # p=0.55
                                               "Orderl_first:r_l_distinction_L1_fdistinct = 0",         # p=0.67
                                               "r_l_distinction_L1_fdistinct:trill_real_L1_fyes = 0")); # p=0.55
  b_L1_full_no_trill_order <- brms_fit_indices(b_L1_full_no_trill_order);
  brms_compare_models(b0,        b_L1_full_no_trill_order, "[null model]", "[L1 full simplified]"); # much better than null
  brms_compare_models(b_L1_full, b_L1_full_no_trill_order, "[L1 full]",    "[L1 full simplified]"); # simplified is better than full...
  
  # - r_l_distinction_L1_f:Order:
  b_L1_full_no_trill_order_rl <- brm(Match ~ 1 + 
                                       Order + # order (we know it has a main effect)
                                       r_l_distinction_L1_f + trill_real_L1_f + # r/l (doesn't have a main effect) and trill (might have a main effect)
                                       r_l_distinction_L1_f:trill_real_L1_f + # do they interact between them?
                                       (1 + Order + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L1_f:trill_real_L1_f | Language) +
                                       (1 + Order + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L1_f:trill_real_L1_f | Family) +
                                       (1 + Order + r_l_distinction_L1_f + trill_real_L1_f + r_l_distinction_L1_f:trill_real_L1_f | Autotyp_Area),
                                     data = web,
                                     family=bernoulli(link='logit'),
                                     prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                                             brms::set_prior("lkj(2)", class="cor")),
                                     save_pars=save_pars(all=TRUE), # needed for Bayes factors
                                     sample_prior=TRUE,             # needed for hypotheses tests
                                     seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_L1_full_no_trill_order_rl); mcmc_plot(b_L1_full_no_trill_order_rl, type="trace", variable="^b_", regex=TRUE); mcmc_plot(b_L1_full_no_trill_order_rl, variable="^b_", regex=TRUE); # very decent
  brms::hypothesis(b_L1_full_no_trill_order_rl, c("Orderl_first = 0",                                      # p=0.46
                                                  "r_l_distinction_L1_fdistinct = 0",                      # p=0.73
                                                  "trill_real_L1_fyes = 0",                                # p=0.55
                                                  "r_l_distinction_L1_fdistinct:trill_real_L1_fyes = 0")); # p=0.56
  b_L1_full_no_trill_order_rl <- brms_fit_indices(b_L1_full_no_trill_order_rl);
  brms_compare_models(b0,        b_L1_full_no_trill_order_rl, "[null model]", "[L1 full simplified]"); # much better than null
  brms_compare_models(b_L1_full, b_L1_full_no_trill_order_rl, "[L1 full]",    "[L1 full simplified]"); # simplified is better than full...
  
  # - r_l_distinction_L1_f:trill_real_L1_f (aka, main effects only):
  b_L1_full_main_effects <- brm(Match ~ 1 + 
                                  Order + # order (we know it has a main effect)
                                  r_l_distinction_L1_f + trill_real_L1_f + # r/l (doesn't have a main effect) and trill (might have a main effect)
                                  (1 + Order + r_l_distinction_L1_f + trill_real_L1_f | Language) +
                                  (1 + Order + r_l_distinction_L1_f + trill_real_L1_f | Family) +
                                  (1 + Order + r_l_distinction_L1_f + trill_real_L1_f | Autotyp_Area),
                                data = web,
                                family=bernoulli(link='logit'),
                                prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                                        brms::set_prior("lkj(2)", class="cor")),
                                save_pars=save_pars(all=TRUE), # needed for Bayes factors
                                sample_prior=TRUE,             # needed for hypotheses tests
                                seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_L1_full_main_effects); mcmc_plot(b_L1_full_main_effects, type="trace", variable="^b_", regex=TRUE); mcmc_plot(b_L1_full_main_effects, variable="^b_", regex=TRUE); # very decent
  brms::hypothesis(b_L1_full_main_effects, c("Orderl_first = 0",                                      # p=0.46
                                             "r_l_distinction_L1_fdistinct = 0",                      # p=0.73
                                             "trill_real_L1_fyes = 0"));                              # p=0.56
  b_L1_full_main_effects <- brms_fit_indices(b_L1_full_main_effects);
  brms_compare_models(b0,        b_L1_full_main_effects, "[null model]", "[L1 full simplified]"); # much better than null
  brms_compare_models(b_L1_full, b_L1_full_main_effects, "[L1 full]",    "[L1 full simplified]"); # simplified is better than full...
  
  
  ## save all these results for later:
  # save the models( if requested):
  if( .save_models )
  {
    web_regressions_L1_models <- list("glmer"=list("null"  =m0,
                                                   "order" =m_Order,
                                                   "sex"   =m_Sex,
                                                   "age"   =m_Age,
                                                   "rl"    =m_rl,
                                                   "trill" =m_trill,
                                                   "rl2"   =m_rl2,
                                                   "trill2"=m_trill2),
                                      "brms"=list("null"  =b0,
                                                  "order" =b_Order,
                                                  "sex"   =b_Sex,
                                                  "age"   =b_Age,
                                                  "rl"    =b_lr,
                                                  "trill" =b_trill,
                                                  "rl2"   =b_lr2,
                                                  "trill2"=b_trill2));
    save(web_regressions_L1_models, file="./models/web_regressions_L1_models.rds", compress="xz", compression_level=9);
  }
  # save the summaries (much less disk space but enough info to undertsand the results):
  web_regressions_L1_summaries <- list("glmer"=list("vif"   =performance::check_collinearity(m_vif), 
                                                    "vif2"  =performance::check_collinearity(m_vif2),
                                                    "icc"   =icc(m0),
                                                    "null"  =list("summary"=summary(m0),
                                                                  "CI95"=confint(m0, method="Wald")),
                                                    "order" =list("summary"=summary(m_Order),
                                                                  "CI95"=confint(m_Order, method="Wald"),
                                                                  "anova_0"=anova(m_Order, m0)),
                                                    "sex"   =list("summary"=summary(m_Sex),
                                                                  "CI95"=confint(m_Sex, method="Wald"),
                                                                  "anova_0"=anova(m_Sex,   m0)),
                                                    "age"   =list("summary"=summary(m_Age),
                                                                  "CI95"=confint(m_Age, method="Wald"),
                                                                  "anova_0"=anova(m_Age,   m0)),
                                                    "rl"    =list("summary"=summary(m_rl),
                                                                  "CI95"=confint(m_rl, method="Wald"),
                                                                  "anova_0"=anova(m_rl,    m0)),
                                                    "trill" =list("summary"=summary(m_trill),
                                                                  "CI95"=confint(m_trill, method="Wald"),
                                                                  "anova_0"=anova(m_trill, m0)),
                                                    "rl2"   =list("summary"=summary(m_rl2),
                                                                  "CI95"=confint(m_rl2, method="Wald"),
                                                                  "anova_0"=anova(m_rl2,    update(m0, . ~ ., data=web[ !is.na(web$r_l_distinction_L2_f), ]))),
                                                    "trill2"=list("summary"=summary(m_trill2),
                                                                  "CI95"=confint(m_trill2, method="Wald"),
                                                                  "anova_0"=anova(m_trill2, update(m0, . ~ ., data=web[ !is.na(web$trill_real_L2_f), ])))),
                                       "brms"=list("prior_pred_check"=list("plot_path"="./plots/web_L1_prior_predictive_checks.jpg"),
                                                   "null" =list("plot_prefix"="./plots/web_L1_null", 
                                                                "hdi"=bayestestR::hdi(b0, ci=0.95),
                                                                "rope"=bayestestR::rope(b0, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                "fixef"=fixef(b0), "ranef"=ranef(b0),
                                                                "summary"=capture.output(summary(b0))),
                                                   "order" =list("plot_prefix"="./plots/web_L1_order", 
                                                                 "hdi"=bayestestR::hdi(b_Order, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_Order, c("Orderl_first = 0")),
                                                                 "rope"=bayestestR::rope(b_Order, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_Order), "ranef"=ranef(b_Order),
                                                                 "summary"=capture.output(summary(b_Order)),
                                                                 "cmp_0"=brms_compare_models(b0, b_Order, "[null model]", "[+ order]")),
                                                   "sex"   =list("plot_prefix"="./plots/web_L1_sex", 
                                                                 "hdi"=bayestestR::hdi(b_Sex, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_Sex, c("SexM = 0")),
                                                                 "rope"=bayestestR::rope(b_Sex, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_Sex), "ranef"=ranef(b_Sex),
                                                                 "summary"=capture.output(summary(b_Sex)),
                                                                 "cmp_0"=brms_compare_models(b0, b_Sex,   "[null model]", "[+ sex]")),
                                                   "age"   =list("plot_prefix"="./plots/web_L1_age", 
                                                                 "hdi"=bayestestR::hdi(b_Age, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_Age, c("Age = 0")),
                                                                 "rope"=bayestestR::rope(b_Age, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_Age), "ranef"=ranef(b_Age),
                                                                 "summary"=capture.output(summary(b_Age)),
                                                                 "cmp_0"=brms_compare_models(b0, b_Age,   "[null model]", "[+ age]")),
                                                   "rl"    =list("plot_prefix"="./plots/web_L1_lrdistinction", 
                                                                 "hdi"=bayestestR::hdi(b_lr, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_lr, c("r_l_distinction_L1_fdistinct = 0")),
                                                                 "rope"=bayestestR::rope(b_lr, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_lr), "ranef"=ranef(b_lr),
                                                                 "summary"=capture.output(summary(b_lr)),
                                                                 "cmp_0"=brms_compare_models(b0, b_lr,    "[null model]", "[+ r/l distinction]")),
                                                   "trill" =list("plot_prefix"="./plots/web_L1_trill", 
                                                                 "hdi"=bayestestR::hdi(b_trill, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_trill, c("trill_real_L1_fyes = 0")),
                                                                 "rope"=bayestestR::rope(b_trill, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_trill), "ranef"=ranef(b_trill),
                                                                 "summary"=capture.output(summary(b_trill)),
                                                                 "cmp_0"=brms_compare_models(b0, b_trill, "[null model]", "[+ trill]")),
                                                   "rl2"   =list("plot_prefix"="./plots/web_L2_lrdistinction", 
                                                                 "hdi"=bayestestR::hdi(b_lr2, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_lr2, c("r_l_distinction_L2_fdistinct = 0")),
                                                                 "rope"=bayestestR::rope(b_lr2, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_lr2), "ranef"=ranef(b_lr2),
                                                                 "summary"=capture.output(summary(b_lr2)),
                                                                 "cmp_0"=NULL),
                                                   "trill2"=list("plot_prefix"="./plots/web_L2_trill", 
                                                                 "hdi"=bayestestR::hdi(b_trill2, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_trill2, c("trill_real_L2_fyes = 0")),
                                                                 "rope"=bayestestR::rope(b_trill2, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_trill2), "ranef"=ranef(b_trill2),
                                                                 "summary"=capture.output(summary(b_trill2)),
                                                                 "cmp_0"=NULL)));
  save(web_regressions_L1_summaries, file="./models/web_regressions_L1_summaries.rds", compress="xz", compression_level=9);
} else
{
  #if( .save_models && file.exists("./models/web_regressions_L1_models.rds") ) load("./models/web_regressions_L1_models.rds") else web_regressions_L1_models <- NULL;
  if( file.exists("./models/web_regressions_L1_summaries.rds") ) load("./models/web_regressions_L1_summaries.rds") else web_regressions_L1_summaries <- NULL; 
}
```


## The probability of a perfect match in the absence of any predictors 

First, we determined the probability of a perfect match (i.e., both responses are correct) without any predictors (aka the null model).

For clarity, the null model is:

```
Match ~ 1 + 
  (1 | Language) +
  (1 | Family) +
  (1 | Autotyp_Area)
```

and we are interested in the intercept, which represents the probability of a match.


### Frequentist

```{r}
print(web_regressions_L1_summaries$glmer$null$summary);
```

The intercept is clearly and significantly > 0 (`r tmp <- web_regressions_L1_summaries$glmer$null; sprintf("*p*=%s", scinot(tmp$summary$coefficients["(Intercept)", "Pr(>|z|)"]))`) and translates into a probability of a match `r sprintf("*p*(match)=%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100*lo2p(tmp$summary$coefficients["(Intercept)", "Estimate"]), 100*lo2p(tmp$CI95["(Intercept)",1]), 100*lo2p(tmp$CI95["(Intercept)",2]))`, much higher than 50%.

The ICC of *match* estimated from the null model is `r sprintf("%.1f%%", 100*web_regressions_L1_summaries$glmer$icc$ICC_adjusted)`.


### Bayesian

```{r}
cat(paste0(web_regressions_L1_summaries$brms$null$summary, collapse="\n"));
```

The intercept is clearly and significantly > 0 (`r tmp <- web_regressions_L1_summaries$brms$null; sprintf("%% inside ROPE = %.1f%%", tmp$rope$ROPE_Percentage[ tmp$rope$Parameter == "b_Intercept" ])`) and translates into a probability of a match `r sprintf("*p*(match)=%.1f%% 95%%HDI [%.1f%%, %.1f%%]", 100*lo2p(tmp$fixef["Intercept", "Estimate"]), 100*lo2p(tmp$hdi$CI_low[ tmp$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmp$hdi$CI_high[ tmp$hdi$Parameter == "b_Intercept" ]))`, much higher than 50%.

The **prior predictive checks** support the choice of priors:
<center>![Prior predictive checks. The observed values (*y*) should fall within the distribution of the simulated priori distribution (*y*~rep~) for the minimum, mean and maximum values.](`r web_regressions_L1_summaries$brms$prior_pred_check$plot_path`){width="4in"}</center>

The model **converges** well:
<center>![Traces of the Bayesian fitting processes.](`r paste0(web_regressions_L1_summaries$brms$null$plot_prefix,"_mcmctrace.jpg")`){width="6in"}</center>

and the **results** show a positive effect of *rough* (but the 95%HDI does include 0)::
<center>![Posterior estimates (medians) with 50% and 90% posterior probability intervals.](`r paste0(web_regressions_L1_summaries$brms$null$plot_prefix,"_mcmcestim.jpg")`){width="5in"}</center>

The **posterior predictive checks** seem ok:
<center>![Posterior predictive checks: density overlays of the observed (*y*) and simulated (*y*~rep~) data.](`r paste0(web_regressions_L1_summaries$brms$null$plot_prefix,"_ppchecks_densoverlay.jpg")`){width="6in"}</center>
<center>![Posterior predictive checks: minimum, mean and maximum values of the observed (*y*) and simulated (*y*~rep~) data.](`r paste0(web_regressions_L1_summaries$brms$null$plot_prefix,"_ppchecks_min_mean_max.jpg")`){width="4in"}</center>


## The potential predictors individually

Then, we checked if any of the potential predictors adds anything to the null model.
(Please note that we show only the relevant information to keep this document simple.)

In the Bayesian approach, we fitted the maximal random effects structure (*x* is the predictor):

```
Match ~ 1 + x +
  (1 + x | Language) +
  (1 + x | Family) +
  (1 + x | Autotyp_Area)
```

but we had to drastically simplify it for the frequentist models to achieve convergence (as indicated in each case).

We show them both as log odds ratios and as probabilities, as appropriate.
We use a tabular presentation combing the frequentist ("ML") and Bayesian ("B") approaches and showing, as appropriate, the estimate with its 95%CI or 95%HDI, the *p*-value or the proportion inside the ROPE and the *p*(=0) of the Bayesian formal hypothesis testing, and the model comparison versus the null as the *χ*^2^ test and ΔAIC(model - null) or the Bayes factor (BF), ΔLOO, ΔWAIC and ΔKFOLD (with the standard deviation).


### Order

Frequentist random effects is `(1 | Language)`. `r tmpg <- web_regressions_L1_summaries$glmer$order; tmpb <- web_regressions_L1_summaries$brms$order`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["(Intercept)", "Estimate"], tmpg$CI95["(Intercept)",1], tmpg$CI95["(Intercept)",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["(Intercept)", "Estimate"]), 100*lo2p(tmpg$CI95["(Intercept)",1]), 100*lo2p(tmpg$CI95["(Intercept)",2]))` | `r scinot(tmpg$summary$coefficients["(Intercept)", "Pr(>|z|)"])` |  |
|                 | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *order* (*β*~l_first-r_first~)   | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["Orderl_first", "Estimate"], tmpg$CI95["Orderl_first",1], tmpg$CI95["Orderl_first",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["Orderl_first", "Estimate"]), 100*lo2p(tmpg$CI95["Orderl_first",1]), 100*lo2p(tmpg$CI95["Orderl_first",2]))` | `r scinot(tmpg$summary$coefficients["Orderl_first", "Pr(>|z|)"])` | `r sprintf("χ^2^(%d)=%.2f, *p*=%s, ΔAIC=%.1f", tmpg$anova_0$Df[2], tmpg$anova_0$Chisq[2], scinot(tmpg$anova_0[2,"Pr(>Chisq)"]), tmpg$anova_0$AIC[2] - tmpg$anova_0$AIC[1])` |
| *order* (*β*~l_first-r_first~)   | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Orderl_first", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Orderl_first" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Orderl_first" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Orderl_first", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Orderl_first" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Orderl_first" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Orderl_first" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | `r .print.model.comparison(b=tmpb$cmp_0)` |


### Sex

Frequentist random effects is `(1 | Language)`. `r tmpg <- web_regressions_L1_summaries$glmer$sex; tmpb <- web_regressions_L1_summaries$brms$sex`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["(Intercept)", "Estimate"], tmpg$CI95["(Intercept)",1], tmpg$CI95["(Intercept)",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["(Intercept)", "Estimate"]), 100*lo2p(tmpg$CI95["(Intercept)",1]), 100*lo2p(tmpg$CI95["(Intercept)",2]))` | `r scinot(tmpg$summary$coefficients["(Intercept)", "Pr(>|z|)"])` |  |
|                 | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *sex* (*β*~M-F~) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["SexM", "Estimate"], tmpg$CI95["SexM",1], tmpg$CI95["SexM",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["SexM", "Estimate"]), 100*lo2p(tmpg$CI95["SexM",1]), 100*lo2p(tmpg$CI95["SexM",2]))` | `r scinot(tmpg$summary$coefficients["SexM", "Pr(>|z|)"])` | `r sprintf("χ^2^(%d)=%.2f, *p*=%s, ΔAIC=%.1f", tmpg$anova_0$Df[2], tmpg$anova_0$Chisq[2], scinot(tmpg$anova_0[2,"Pr(>Chisq)"]), tmpg$anova_0$AIC[2] - tmpg$anova_0$AIC[1])` |
| *sex* (*β*~M-F~) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["SexM", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_SexM" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_SexM" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["SexM", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_SexM" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_SexM" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_SexM" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | `r .print.model.comparison(b=tmpb$cmp_0)` |


### Age

Frequentist random effects is `(1 + Age | Language)`. `r tmpg <- web_regressions_L1_summaries$glmer$age; tmpb <- web_regressions_L1_summaries$brms$age`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["(Intercept)", "Estimate"], tmpg$CI95["(Intercept)",1], tmpg$CI95["(Intercept)",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["(Intercept)", "Estimate"]), 100*lo2p(tmpg$CI95["(Intercept)",1]), 100*lo2p(tmpg$CI95["(Intercept)",2]))` | `r scinot(tmpg$summary$coefficients["(Intercept)", "Pr(>|z|)"])` |  |
|                 | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *age* (*β*)     | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["Age", "Estimate"], tmpg$CI95["Age",1], tmpg$CI95["Age",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["Age", "Estimate"]), 100*lo2p(tmpg$CI95["Age",1]), 100*lo2p(tmpg$CI95["Age",2]))` | `r scinot(tmpg$summary$coefficients["Age", "Pr(>|z|)"])` | `r sprintf("χ^2^(%d)=%.2f, *p*=%s, ΔAIC=%.1f", tmpg$anova_0$Df[2], tmpg$anova_0$Chisq[2], scinot(tmpg$anova_0[2,"Pr(>Chisq)"]), tmpg$anova_0$AIC[2] - tmpg$anova_0$AIC[1])` |
| *age* (*β*)     | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Age", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Age" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Age" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Age", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Age" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Age" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Age" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | `r .print.model.comparison(b=tmpb$cmp_0)` |


### R/L distinction in L1?

Frequentist random effects is `(1 + r_l_distinction_L1_f | Language)`. `r tmpg <- web_regressions_L1_summaries$glmer$rl; tmpb <- web_regressions_L1_summaries$brms$rl`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["(Intercept)", "Estimate"], tmpg$CI95["(Intercept)",1], tmpg$CI95["(Intercept)",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["(Intercept)", "Estimate"]), 100*lo2p(tmpg$CI95["(Intercept)",1]), 100*lo2p(tmpg$CI95["(Intercept)",2]))` | `r scinot(tmpg$summary$coefficients["(Intercept)", "Pr(>|z|)"])` |  |
|                 | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *r/l distinction* (*β*~distinct-not~) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["r_l_distinction_L1_fdistinct", "Estimate"], tmpg$CI95["r_l_distinction_L1_fdistinct",1], tmpg$CI95["r_l_distinction_L1_fdistinct",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["r_l_distinction_L1_fdistinct", "Estimate"]), 100*lo2p(tmpg$CI95["r_l_distinction_L1_fdistinct",1]), 100*lo2p(tmpg$CI95["r_l_distinction_L1_fdistinct",2]))` | `r scinot(tmpg$summary$coefficients["r_l_distinction_L1_fdistinct", "Pr(>|z|)"])` | `r sprintf("χ^2^(%d)=%.2f, *p*=%s, ΔAIC=%.1f", tmpg$anova_0$Df[2], tmpg$anova_0$Chisq[2], scinot(tmpg$anova_0[2,"Pr(>Chisq)"]), tmpg$anova_0$AIC[2] - tmpg$anova_0$AIC[1])` |
| *r/l distinction* (*β*~distinct-not~) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["r_l_distinction_L1_fdistinct", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_r_l_distinction_L1_fdistinct" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_r_l_distinction_L1_fdistinct" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["r_l_distinction_L1_fdistinct", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_r_l_distinction_L1_fdistinct" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_r_l_distinction_L1_fdistinct" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_r_l_distinction_L1_fdistinct" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | `r .print.model.comparison(b=tmpb$cmp_0)` |


### [r] in L1?

Frequentist random effects is `(1 | Language)`. `r tmpg <- web_regressions_L1_summaries$glmer$trill; tmpb <- web_regressions_L1_summaries$brms$trill`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["(Intercept)", "Estimate"], tmpg$CI95["(Intercept)",1], tmpg$CI95["(Intercept)",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["(Intercept)", "Estimate"]), 100*lo2p(tmpg$CI95["(Intercept)",1]), 100*lo2p(tmpg$CI95["(Intercept)",2]))` | `r scinot(tmpg$summary$coefficients["(Intercept)", "Pr(>|z|)"])` |  |
|                 | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *[r]* (*β*~yes-no~) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["trill_real_L1_fyes", "Estimate"], tmpg$CI95["trill_real_L1_fyes",1], tmpg$CI95["trill_real_L1_fyes",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["trill_real_L1_fyes", "Estimate"]), 100*lo2p(tmpg$CI95["trill_real_L1_fyes",1]), 100*lo2p(tmpg$CI95["trill_real_L1_fyes",2]))` | `r scinot(tmpg$summary$coefficients["trill_real_L1_fyes", "Pr(>|z|)"])` | `r sprintf("χ^2^(%d)=%.2f, *p*=%s, ΔAIC=%.1f", tmpg$anova_0$Df[2], tmpg$anova_0$Chisq[2], scinot(tmpg$anova_0[2,"Pr(>Chisq)"]), tmpg$anova_0$AIC[2] - tmpg$anova_0$AIC[1])` |
| *[r]* (*β*~yes-no~) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["trill_real_L1_fyes", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_trill_real_L1_fyes" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_trill_real_L1_fyes" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["trill_real_L1_fyes", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_trill_real_L1_fyes" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_trill_real_L1_fyes" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_trill_real_L1_fyes" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | `r .print.model.comparison(b=tmpb$cmp_0)` |


### R/L distinction in L2?

Frequentist random effects is `(1 + r_l_distinction_L2_f | Language)`. `r tmpg <- web_regressions_L1_summaries$glmer$rl2; tmpb <- web_regressions_L1_summaries$brms$rl2`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["(Intercept)", "Estimate"], tmpg$CI95["(Intercept)",1], tmpg$CI95["(Intercept)",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["(Intercept)", "Estimate"]), 100*lo2p(tmpg$CI95["(Intercept)",1]), 100*lo2p(tmpg$CI95["(Intercept)",2]))` | `r scinot(tmpg$summary$coefficients["(Intercept)", "Pr(>|z|)"])` |  |
|                 | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *r/l distinction L2* (*β*~distinct-not~) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["r_l_distinction_L2_fdistinct", "Estimate"], tmpg$CI95["r_l_distinction_L2_fdistinct",1], tmpg$CI95["r_l_distinction_L2_fdistinct",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["r_l_distinction_L2_fdistinct", "Estimate"]), 100*lo2p(tmpg$CI95["r_l_distinction_L2_fdistinct",1]), 100*lo2p(tmpg$CI95["r_l_distinction_L2_fdistinct",2]))` | `r scinot(tmpg$summary$coefficients["r_l_distinction_L2_fdistinct", "Pr(>|z|)"])` | `r sprintf("χ^2^(%d)=%.2f, *p*=%s, ΔAIC=%.1f", tmpg$anova_0$Df[2], tmpg$anova_0$Chisq[2], scinot(tmpg$anova_0[2,"Pr(>Chisq)"]), tmpg$anova_0$AIC[2] - tmpg$anova_0$AIC[1])` |
| *r/l distinction L2* (*β*~distinct-not~) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["r_l_distinction_L2_fdistinct", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_r_l_distinction_L2_fdistinct" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_r_l_distinction_L2_fdistinct" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["r_l_distinction_L2_fdistinct", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_r_l_distinction_L2_fdistinct" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_r_l_distinction_L2_fdistinct" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_r_l_distinction_L2_fdistinct" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | not performed due to high computational load and predictable outcome |


### [r] in L2?

Frequentist random effects is `(1 + trill_real_L2_f | Language)`. `r tmpg <- web_regressions_L1_summaries$glmer$trill2; tmpb <- web_regressions_L1_summaries$brms$trill2`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["(Intercept)", "Estimate"], tmpg$CI95["(Intercept)",1], tmpg$CI95["(Intercept)",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["(Intercept)", "Estimate"]), 100*lo2p(tmpg$CI95["(Intercept)",1]), 100*lo2p(tmpg$CI95["(Intercept)",2]))` | `r scinot(tmpg$summary$coefficients["(Intercept)", "Pr(>|z|)"])` |  |
|                 | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *[r]* (*β*~yes-no~) | ML | `r sprintf("%.2f [%.2f, %.2f]", tmpg$summary$coefficients["trill_real_L2_fyes", "Estimate"], tmpg$CI95["trill_real_L2_fyes",1], tmpg$CI95["trill_real_L2_fyes",2])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpg$summary$coefficients["trill_real_L2_fyes", "Estimate"]), 100*lo2p(tmpg$CI95["trill_real_L2_fyes",1]), 100*lo2p(tmpg$CI95["trill_real_L2_fyes",2]))` | `r scinot(tmpg$summary$coefficients["trill_real_L2_fyes", "Pr(>|z|)"])` | `r sprintf("χ^2^(%d)=%.2f, *p*=%s, ΔAIC=%.1f", tmpg$anova_0$Df[2], tmpg$anova_0$Chisq[2], scinot(tmpg$anova_0[2,"Pr(>Chisq)"]), tmpg$anova_0$AIC[2] - tmpg$anova_0$AIC[1])` |
| *[r]* (*β*~yes-no~) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["trill_real_L2_fyes", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_trill_real_L2_fyes" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_trill_real_L2_fyes" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["trill_real_L2_fyes", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_trill_real_L2_fyes" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_trill_real_L2_fyes" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_trill_real_L2_fyes" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | not performed due to high computational load and predictable outcome |


### Summary

In all cases the intercept *α* is highly significant and positive, comparable with the null model at ≥ 80%, so much higher than 50%.

On the other hand, of all the potential predictors only *order* is formally significantly adding to the null model, with "l first" *decreasing* the probability of a match by ≈28%.

Overall, the frequentist methods cannot accommodate the full random effects structure.
Moreover, ignoring on purpose the random slopes of "r/l distinction" and "exists [r]" artificially inflates the corresponding fixed effects.

As an extra check, we also started from a full model containing all the potential predictors and their meaningful interactions, but this simplifies as expected from the above simple regressions.



# Field experiment

## Descriptive statistics

First, how many participants?

```{r}
nrow(field)
```

Sex division

```{r}
table(field$Sex)
```

Ages

```{r}
summary(field$Age)
```

First, how many languages?

```{r}
field %>% count(Language) %>% nrow()
```

Does this number correspond with the L1s?

```{r}
field %>% count(Name) %>% nrow()
```

How many families?

```{r}
field %>% count(Family) %>% nrow()
```

How many have the R/L distinction in the L1 among the languages?

```{r}
field %>% count(Name, r_l_distinction_L1) %>% count(r_l_distinction_L1)
```

How many really use the alveolar trill in L1 among the languages?

```{r}
field %>% count(Name, trill_real_L1) %>% count(trill_real_L1)
```

How many really have the alveolar trill in L1 as an allophone among the
languages?

```{r}
field %>% count(Name, trill_occ_L1) %>% count(trill_occ_L1)
```

What about the same questions for L2. But this will not neatly sum up to
25, due to various possible scenarios for L2 within a specific L1.

How many have the R/L distinction in the L2 among the languages?

```{r}
field %>% count(Name, r_l_distinction_L2) %>% count(r_l_distinction_L2)
```

How many really use the alveolar trill in L2 among the languages?

```{r}
field %>% count(Name, trill_real_L2) %>% count(trill_real_L2)
```

How many really have the alveolar trill in L2 as an allophone among the
languages?

```{r}
field %>% count(Name, trill_occ_L2) %>% count(trill_occ_L2)
```

What is the grand average congruent behavior?

```{r}
mean(field$Match)
```

97%!!!

What about only among those who have L1 without the distinction?

```{r}
field %>%
  filter(r_l_distinction_L1 == "0") %>%
  summarize(mean_match = mean(Match, na.rm = TRUE))
```

100%! WOW.

What about only among those who have L1 without the distinction and no
L2 that distinguishes?

```{r}
field %>%
  filter(r_l_distinction_L1 == "0") %>%
  filter(!EnglishL2YesNo) %>% 
  filter(r_l_distinction_L2 == '0') %>% 
  summarize(mean_match = mean(Match, na.rm = TRUE))
```

There are no such people.

Compute average matching behavior per language and sort:

```{r}
field_avg <- field %>%
  group_by(Language) %>% 
  summarize(M = mean(Match)) %>% 
  arrange(desc(M)) %>% 
  mutate(percent = round(M, 2) * 100,
         percent = str_c(percent, '%'))

# Show:

field_avg %>% print(n = Inf)
```

Check some demographics, also to report in the paper. First, the number
of participants per language:

```{r}
field %>% 
  count(Name, sort = TRUE) %>% print(n = Inf)
```

Then, the number of L1 speakers who have R/L distinction vs. who don't:

```{r}
field %>% count(r_l_distinction_L1) %>%
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

How many people do not have any L2?

```{r}
# raw count of no L2; raw count of all ppl
sum(is.na(field$L2)); nrow(field)

# percentage no L2
sum(is.na(field$L2)) / nrow(field)

# percentage with L2
1 - sum(is.na(field$L2)) / nrow(field)
```

Check how many people knew English as their L2:

```{r}
field %>% count(EnglishL2YesNo) %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

Of those that don't use a R/L distinction in their L1, how many use R/L
distinction in their L2? (double-check if logic alright!)

```{r}
field %>%
  filter(r_l_distinction_L1 == '0') %>% 
  count(r_l_distinction_L2 == '1') %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

How many "pure" speakers were there?, i.e., those people that 1) don't
know English, 2) don't use an L1 with a R/L distinction, and 3) don't
know an L2 that distinguishes R/L.

```{r}
field %>% 
  filter(r_l_distinction_L1 == '0') %>%  # excludes English as well
  filter(!EnglishL2YesNo) %>% 
  filter(r_l_distinction_L2 == '0') %>% 
  nrow()
```

None.


## Regression models

Check the distribution of scripts across families to make decisions
about random effects structure:

```{r}
table(field$Family, field$r_l_distinction_L1)
```

@Dan & @Bodo, can you please check if we need some manipulation, like
contrast-coding but weighted? Bodo, I know for bouba/kiki you did some
kind of weighted modification and I'm sure that the proportions of our
predictors are not balanced, see below:

```{r}
field %>% count(r_l_distinction_L1) %>%
  mutate(prop = n / sum(n))
# highly imbalanced

field %>% count(trill_real_L1) %>%
  mutate(prop = n / sum(n))

field %>% count(trill_occ_L1) %>%
  mutate(prop = n / sum(n))
# highly imbalanced

## And for L2, just in case
field %>% count(r_l_distinction_L2) %>%
  mutate(prop = n / sum(n))

field %>% count(trill_real_L2) %>%
  mutate(prop = n / sum(n))

field %>% count(trill_occ_L2) %>%
  mutate(prop = n / sum(n))

# Code them as factors:
field$r_l_distinction_L1_f <- factor(c("same", "distinct")[field$r_l_distinction_L1 + 1], levels=c("same", "distinct"));
field$trill_real_L1_f      <- factor(c("no", "yes")[field$trill_real_L1 + 1], levels=c("no", "yes"));
field$trill_occ_L1_f       <- factor(c("no", "yes")[field$trill_occ_L1 + 1], levels=c("no", "yes"));
field$r_l_distinction_L2_f <- factor(c("same", "distinct")[field$r_l_distinction_L2 + 1], levels=c("same", "distinct"));
field$trill_real_L2_f      <- factor(c("no", "yes")[field$trill_real_L2 + 1], levels=c("no", "yes"));
field$trill_occ_L2_f       <- factor(c("no", "yes")[field$trill_occ_L2 + 1], levels=c("no", "yes"));
```


```{r, message = FALSE, warning = FALSE}

if( !file.exists("./models/field_regressions_summaries.rds") ||
    !(.save_models & file.exists("./models/field_regressions_models.rds")) )
{
  # Actually fit the models and save various summaries (and potentially the actual models as well)
  # It's pretty computationally expensive, especially the Bayesian ones!
  
  # with glmer --> cannot model random effects!

  # with brms:
  
  b0 <- brm(Match ~ 1 + 
              (1 | Language),
            data = field,
            family=bernoulli(link='logit'),
            save_pars=save_pars(all=TRUE), # needed for Bayes factors
            sample_prior=TRUE,  # needed for hypotheses tests
            seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.999, max_treedepth=13));
  summary(b0); mcmc_plot(b0, type="trace"); mcmc_plot(b0); # very decent
  bayestestR::hdi(b0, ci=0.95);
  b0 <- brms_fit_indices(b0);
  # posterior predictive checks
  plot_prefix <- "./plots/field_null"; b <- b0;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  
  b_Sex <- brm(Match ~ 1 + Sex +
                 (1 + Sex | Language),
               data = field,
               family=bernoulli(link='logit'),
               prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                       brms::set_prior("lkj(2)", class="cor")),
               save_pars=save_pars(all=TRUE), # needed for Bayes factors
               sample_prior=TRUE,             # needed for hypotheses tests
               seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.999, max_treedepth=13));
  summary(b_Sex); mcmc_plot(b_Sex, type="trace"); mcmc_plot(b_Sex); # very decent
  brms::hypothesis(b_Sex, c("Sexm = 0")); # p=0.84
  b_Sex <- brms_fit_indices(b_Sex);
  brms_compare_models(b0, b_Sex, "[null model]", "[+ sex]"); # sex does not matter
  # posterior predictive checks
  plot_prefix <- "./plots/field_sex"; b <- b_Sex;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="Sex"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  b_Age <- brm(Match ~ 1 + Age +
                 (1 + Age | Language),
               data = field,
               family=bernoulli(link='logit'),
               prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                       brms::set_prior("lkj(2)", class="cor")),
               save_pars=save_pars(all=TRUE), # needed for Bayes factors
               sample_prior=TRUE,             # needed for hypotheses tests
               seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.999, max_treedepth=13));
  summary(b_Age); mcmc_plot(b_Age, type="trace"); mcmc_plot(b_Age); # very decent
  brms::hypothesis(b_Age, c("Age = 0")); # p=0.96
  b_Age <- brms_fit_indices(b_Age);
  brms_compare_models(b0, b_Age, "[null model]", "[+ age]"); # age is worse than null
  # posterior predictive checks
  plot_prefix <- "./plots/field_age"; b <- b_Age;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="Age"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  b_lr <- brm(Match ~ 1 + r_l_distinction_L1_f +
                (1 + r_l_distinction_L1_f | Language),
              data = field,
              family=bernoulli(link='logit'),
              prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                      brms::set_prior("lkj(2)", class="cor")),
              save_pars=save_pars(all=TRUE), # needed for Bayes factors
              sample_prior=TRUE,             # needed for hypotheses tests
              seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_lr); mcmc_plot(b_lr, type="trace"); mcmc_plot(b_lr); # very decent
  brms::hypothesis(b_lr, c("r_l_distinction_L1_fdistinct = 0")); # p=0.54
  b_lr <- brms_fit_indices(b_lr);
  brms_compare_models(b0, b_lr, "[null model]", "[+ l/r distinction]"); # l/r distinction is worse than null
  # posterior predictive checks
  plot_prefix <- "./plots/field_lrdistinction"; b <- b_lr;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="r_l_distinction_L1_f"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  b_trill <- brm(Match ~ 1 + trill_real_L1_f +
                   (1 + trill_real_L1_f | Language),
                 data = field,
                 family=bernoulli(link='logit'),
                 prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                         brms::set_prior("lkj(2)", class="cor")),
                 save_pars=save_pars(all=TRUE), # needed for Bayes factors
                 sample_prior=TRUE,             # needed for hypotheses tests
                 seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_trill); mcmc_plot(b_trill, type="trace"); mcmc_plot(b_trill); # very decent
  brms::hypothesis(b_trill, c("trill_real_L1_fyes = 0")); # p=0.51
  b_trill <- brms_fit_indices(b_trill);
  brms_compare_models(b0, b_trill, "[null model]", "[+ trill]"); # trill is marginally better than the null
  # posterior predictive checks
  plot_prefix <- "./plots/field_trill"; b <- b_trill;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="trill_real_L1_f"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  # L2:
  b_lr2 <- brm(Match ~ 1 + r_l_distinction_L2_f +
                 (1 + r_l_distinction_L2_f | Language),
               data = field,
               family=bernoulli(link='logit'),
               prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                       brms::set_prior("lkj(2)", class="cor")),
               save_pars=save_pars(all=TRUE), # needed for Bayes factors
               sample_prior=TRUE,             # needed for hypotheses tests
               seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_lr2); mcmc_plot(b_lr2, type="trace"); mcmc_plot(b_lr2); # very decent
  brms::hypothesis(b_lr2, c("r_l_distinction_L2_fdistinct = 0")); # p=0.51
  #b_lr2 <- brms_fit_indices(b_lr2);
  #brms_compare_models(b0, b_lr2, "[null model]", "[+ l/r distinction in L2]"); # <-- needs refitting the null on the restricted data
  # posterior predictive checks
  plot_prefix <- "./plots/field_L2_lrdistinction"; b <- b_lr2;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="r_l_distinction_L2_f"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);
  
  b_trill2 <- brm(Match ~ 1 + trill_real_L2_f +
                    (1 + trill_real_L2_f | Language),
                  data = field,
                  family=bernoulli(link='logit'),
                  prior=c(brms::set_prior("student_t(5, 0, 2.5)", class="b"), 
                          brms::set_prior("lkj(2)", class="cor")),
                  save_pars=save_pars(all=TRUE), # needed for Bayes factors
                  sample_prior=TRUE,             # needed for hypotheses tests
                  seed=998, cores=brms_ncores, iter=10000, warmup=4000, thin=2, control=list(adapt_delta=0.9999, max_treedepth=13));
  summary(b_trill2); mcmc_plot(b_trill2, type="trace"); mcmc_plot(b_trill2); # very decent
  brms::hypothesis(b_trill2, c("trill_real_L2_fyes = 0")); # p=0.53
  #b_trill2 <- brms_fit_indices(b_trill2);
  #brms_compare_models(b0, b_trill2, "[null model]", "[+ trill in L2]"); # trill might be better than null
  # posterior predictive checks
  plot_prefix <- "./plots/field_L2_trill"; b <- b_trill2;
  mcmc_plot(b, type="trace"); ggsave(paste0(plot_prefix,"_mcmctrace.jpg"), device="jpeg", width=7, height=6, units="in", quality=85);
  mcmc_plot(b); ggsave(paste0(plot_prefix,"_mcmcestim.jpg"), device="jpeg", width=7, height=4, units="in", quality=85);
  pp_check(b, ndraws=100) + xlab('p(match)') + ggtitle('Posterior predictive density overlay'); ggsave(paste0(plot_prefix,"_ppchecks_densoverlay.jpg"), device="jpeg", width=6, height=4, units="in", quality=85);
  g <- arrangeGrob(pp_check(b, type='stat', sta ='min') + xlab('p(match)') + ggtitle('Posterior predictive distribution of minimum values'),
                   pp_check(b, type='stat', stat='mean') + xlab('p(match)') + ggtitle('Posterior predictive distribution of means'),
                   pp_check(b, type='stat', stat='max') + xlab('p(match)') + ggtitle('Posterior predictive distribution of maximum values'),
                   ncol=1); # seems fine (the observed, y, does fall in the predicted distributions, y_{rep} for the max, mean and min)
  as_ggplot(g); ggsave(paste0(plot_prefix,"_ppchecks_min_mean_max.jpg"), plot=g, device="jpeg", width=6, height=4*3, units="in", quality=85);
  plot_model(b, type="pred", terms="trill_real_L2_f"); ggsave(paste0(plot_prefix,"_pred.jpg"), device="jpeg", width=4, height=4, units="in", quality=85);

  
  ## save all these results for later:
  # save the models( if requested):
  if( .save_models )
  {
    field_regressions_models <- list("brms"=list("null"  =b0,
                                                 "order" =b_Order,
                                                 "sex"   =b_Sex,
                                                 "age"   =b_Age,
                                                 "rl"    =b_lr,
                                                 "trill" =b_trill,
                                                 "rl2"   =b_lr2,
                                                 "trill2"=b_trill2));
    save(field_regressions_models, file="./models/field_regressions_models.rds", compress="xz", compression_level=9);
  }
  # save the summaries (much less disk space but enough info to undertsand the results):
  field_regressions_summaries <- list("brms"=list("prior_pred_check"=list("plot_path"="./plots/field_prior_predictive_checks.jpg"),
                                                   "null" =list("plot_prefix"="./plots/field_null", 
                                                                "hdi"=bayestestR::hdi(b0, ci=0.95),
                                                                "rope"=bayestestR::rope(b0, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                "fixef"=fixef(b0), "ranef"=ranef(b0),
                                                                "summary"=capture.output(summary(b0))),
                                                   "sex"   =list("plot_prefix"="./plots/field_sex", 
                                                                 "hdi"=bayestestR::hdi(b_Sex, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_Sex, c("Sexm = 0")),
                                                                 "rope"=bayestestR::rope(b_Sex, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_Sex), "ranef"=ranef(b_Sex),
                                                                 "summary"=capture.output(summary(b_Sex)),
                                                                 "cmp_0"=brms_compare_models(b0, b_Sex,   "[null model]", "[+ sex]")),
                                                   "age"   =list("plot_prefix"="./plots/field_age", 
                                                                 "hdi"=bayestestR::hdi(b_Age, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_Age, c("Age = 0")),
                                                                 "rope"=bayestestR::rope(b_Age, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_Age), "ranef"=ranef(b_Age),
                                                                 "summary"=capture.output(summary(b_Age)),
                                                                 "cmp_0"=brms_compare_models(b0, b_Age,   "[null model]", "[+ age]")),
                                                   "rl"    =list("plot_prefix"="./plots/field_lrdistinction", 
                                                                 "hdi"=bayestestR::hdi(b_lr, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_lr, c("r_l_distinction_L1_fdistinct = 0")),
                                                                 "rope"=bayestestR::rope(b_lr, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_lr), "ranef"=ranef(b_lr),
                                                                 "summary"=capture.output(summary(b_lr)),
                                                                 "cmp_0"=brms_compare_models(b0, b_lr,    "[null model]", "[+ r/l distinction]")),
                                                   "trill" =list("plot_prefix"="./plots/field_trill", 
                                                                 "hdi"=bayestestR::hdi(b_trill, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_trill, c("trill_real_L1_fyes = 0")),
                                                                 "rope"=bayestestR::rope(b_trill, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_trill), "ranef"=ranef(b_trill),
                                                                 "summary"=capture.output(summary(b_trill)),
                                                                 "cmp_0"=brms_compare_models(b0, b_trill, "[null model]", "[+ trill]")),
                                                   "rl2"   =list("plot_prefix"="./plots/field_L2_lrdistinction", 
                                                                 "hdi"=bayestestR::hdi(b_lr2, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_lr2, c("r_l_distinction_L2_fdistinct = 0")),
                                                                 "rope"=bayestestR::rope(b_lr2, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_lr2), "ranef"=ranef(b_lr2),
                                                                 "summary"=capture.output(summary(b_lr2)),
                                                                 "cmp_0"=NULL),
                                                   "trill2"=list("plot_prefix"="./plots/field_L2_trill", 
                                                                 "hdi"=bayestestR::hdi(b_trill2, ci=0.95),
                                                                 "hypotheses"=brms::hypothesis(b_trill2, c("trill_real_L2_fyes = 0")),
                                                                 "rope"=bayestestR::rope(b_trill2, ci=0.95, ci_method="HDI", verbose=FALSE),
                                                                 "fixef"=fixef(b_trill2), "ranef"=ranef(b_trill2),
                                                                 "summary"=capture.output(summary(b_trill2)),
                                                                 "cmp_0"=NULL)));
  save(field_regressions_summaries, file="./models/field_regressions_summaries.rds", compress="xz", compression_level=9);
} else
{
  #if( .save_models && file.exists("./models/field_regressions_models.rds") ) load("./models/field_regressions_models.rds") else field_regressions_models <- NULL;
  if( file.exists("./models/field_regressions_summaries.rds") ) load("./models/field_regressions_summaries.rds") else field_regressions_summaries <- NULL; 
}
```

Please note that we could not fit frequentist models as the random effects structure does not converge.

## The probability of a perfect match in the absence of any predictors 

First, we determined the probability of a perfect match (i.e., both responses are correct) without any predictors (aka the null model).

For clarity, the null model is:

```
Match ~ 1 + 
  (1 | Language)
```

and we are interested in the intercept, which represents the probability of a match.

### Bayesian

```{r}
cat(paste0(field_regressions_summaries$brms$null$summary, collapse="\n"));
```

The intercept is clearly and significantly > 0 (`r tmp <- field_regressions_summaries$brms$null; sprintf("%% inside ROPE = %.1f%%", tmp$rope$ROPE_Percentage[ tmp$rope$Parameter == "b_Intercept" ])`) and translates into a probability of a match `r sprintf("*p*(match)=%.1f%% 95%%HDI [%.1f%%, %.1f%%]", 100*lo2p(tmp$fixef["Intercept", "Estimate"]), 100*lo2p(tmp$hdi$CI_low[ tmp$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmp$hdi$CI_high[ tmp$hdi$Parameter == "b_Intercept" ]))`, much higher than 50%.

The model **converges** well:
<center>![Traces of the Bayesian fitting processes.](`r paste0(field_regressions_summaries$brms$null$plot_prefix,"_mcmctrace.jpg")`){width="6in"}</center>

and the **results** show a positive effect of *rough* (but the 95%HDI does include 0)::
<center>![Posterior estimates (medians) with 50% and 90% posterior probability intervals.](`r paste0(field_regressions_summaries$brms$null$plot_prefix,"_mcmcestim.jpg")`){width="5in"}</center>

The **posterior predictive checks** seem ok:
<center>![Posterior predictive checks: density overlays of the observed (*y*) and simulated (*y*~rep~) data.](`r paste0(field_regressions_summaries$brms$null$plot_prefix,"_ppchecks_densoverlay.jpg")`){width="6in"}</center>
<center>![Posterior predictive checks: minimum, mean and maximum values of the observed (*y*) and simulated (*y*~rep~) data.](`r paste0(field_regressions_summaries$brms$null$plot_prefix,"_ppchecks_min_mean_max.jpg")`){width="4in"}</center>


## The potential predictors individually

Please note that L2 is degenerate and the models do not make any sense.

### Sex

`r tmpb <- field_regressions_summaries$brms$sex`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*)   | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *sex* (*β*~M-F~) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Sexm", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Sexm" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Sexm" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Sexm", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Sexm" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Sexm" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Sexm" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | `r .print.model.comparison(b=tmpb$cmp_0)` |


### Age

`r tmpb <- field_regressions_summaries$brms$age`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *age* (*β*)     | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Age", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Age" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Age" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Age", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Age" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Age" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Age" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | `r .print.model.comparison(b=tmpb$cmp_0)` |


### R/L distinction in L1?

`r tmpb <- field_regressions_summaries$brms$rl`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *r/l distinction* (*β*~distinct-not~) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["r_l_distinction_L1_fdistinct", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_r_l_distinction_L1_fdistinct" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_r_l_distinction_L1_fdistinct" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["r_l_distinction_L1_fdistinct", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_r_l_distinction_L1_fdistinct" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_r_l_distinction_L1_fdistinct" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_r_l_distinction_L1_fdistinct" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | `r .print.model.comparison(b=tmpb$cmp_0)` |


### [r] in L1?

`r tmpb <- field_regressions_summaries$brms$trill`

| variable | method | log odds ratio (LOR) | probability (%) | *p*       | model comparison |
|---------:|-------:|---------------------:|----------------:|----------:|-----------------:|
| intercept (*α*) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["Intercept", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["Intercept", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_Intercept" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_Intercept" ]))` | `r sprintf("%.1f%%", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_Intercept" ])` |  |
| *[r]* (*β*~yes-no~) | B  | `r sprintf("%.2f [%.2f, %.2f]", tmpb$fixef["trill_real_L1_fyes", "Estimate"], tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_trill_real_L1_fyes" ], tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_trill_real_L1_fyes" ])` | `r sprintf("%.1f%% [%.1f%%, %.1f%%]", 100*lo2p(tmpb$fixef["trill_real_L1_fyes", "Estimate"]), 100*lo2p(tmpb$hdi$CI_low[ tmpb$hdi$Parameter == "b_trill_real_L1_fyes" ]), 100*lo2p(tmpb$hdi$CI_high[ tmpb$hdi$Parameter == "b_trill_real_L1_fyes" ]))` | `r sprintf("pROPE=%.1f%%, *p*(*β*=0)=%.2f", tmpb$rope$ROPE_Percentage[ tmpb$rope$Parameter == "b_trill_real_L1_fyes" ], tmpb$hypotheses$hypothesis[1, "Post.Prob"])` | `r .print.model.comparison(b=tmpb$cmp_0)` |


### Summary

In all cases the intercept *α* is highly significant and positive, comparable with the null model at ≥ 98%, so much higher than 50%.

On the other hand, none of the potential predictors seem to make any difference.


# Conclusions

It is clear that there is a clear and very strong association between [r] and the rugged line and between [l] and the continuous line across the board.
On the other hand, no predictor seems to really affect this, except for the order of presentation ("l" first decreases the assication).







# Previous code (FORCED STOP!)

```{r cache=FALSE}
knitr::knit_exit();
```




### L1 Only

The main model with Order and R/L distinction and alveolar trill as the
preferred variant in L1 as fixed effects, as well as by Language and
Family slopes and intercepts.

```{r, message = FALSE, warning = FALSE}
web_mdl_L1Only <- brm(Match ~ 1 + order_num + r_l_distinction_L1 + trill_real_L1 +
                        (1 + order_num|Language) + 
                        (1 + order_num|Family),
                      data = web,
                      family = bernoulli(link = 'logit'),
                      #backend = "cmdstanr",
                      seed = 998,
                      cores = 4,
                      iter = myiter,
                      warmup = mywarmup,
                      control = list(adapt_delta = 0.995,
                                     max_treedepth = 13),
                      file = paste0(models, "web_mdl_L1Only.rds"))




# test whether coding as factors matters:
web_mdl_L1Only_test <- brm(Match ~ 1 + order_num + r_l_distinction_L1 + trill_real_L1 + Sex + Age + 
                             (1 + order_num  | Language) + 
                             (1 + order_num + r_l_distinction_L1 + trill_real_L1 | Family),
                           (1 + order_num + r_l_distinction_L1 + trill_real_L1 | Autotyp_Area),
                           data = web,
                           family = bernoulli(link = 'logit'),
                           #backend = "cmdstanr",
                           seed = 998,
                           cores = 4,
                           iter = myiter,
                           warmup = mywarmup,
                           control = list(adapt_delta = 0.999,
                                          max_treedepth = 13))
summary(web_mdl_L1Only_test); mcmc_plot(web_mdl_L1Only_test, type="trace"); mcmc_plot(web_mdl_L1Only_test);
```

Show the model:

```{r}
web_mdl_L1Only
```

PPcheck fit:

```{r}
pp_check(web_mdl_L1Only, ndraws = 1000)
```

Compute grand averages with 95% lower and upper bound of credible
interval to report:

```{r}
plogis(fixef(web_mdl_L1Only)[1, ][1])
plogis(fixef(web_mdl_L1Only)[1, ][3]) # lwr bound
plogis(fixef(web_mdl_L1Only)[1, ][4]) # upr bound
```

Perform hypothesis tests:

```{r}
hypothesis(web_mdl_L1Only, c('Intercept > 0', 'r_l_distinction_L1 > 0', 'trill_real_L1 < 0', 'order_num < 0'))
```

Meaning:

-   The r/l phoneme distinction within a language does not have an
    impact of matching r to rough picture/surface and l to smooth
    picture/surface. Speakers of languages that do and that do not
    differentiate between r and l are equally good/bad at matching.
-   Having a real alveolar trill as the main r-phoneme in a language has
    a negative (!) impact on matching, that is the languages that use
    the alveolar trill as their primary /r/ are likely to be worse at
    matching in our data.
-   There is an order effect with r_first trials resulting in better
    matching than l_first trials.

Check the descriptive averages for r/l and order:

```{r}
web %>% 
  group_by(r_l_distinction_L1) %>% 
  summarize(prop = mean(Match)) %>% 
  mutate(percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

```{r}
web %>% 
  group_by(trill_real_L1) %>% 
  summarize(prop = mean(Match)) %>% 
  mutate(percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

```{r}
web %>% 
  group_by(Order) %>% 
  summarize(prop = mean(Match)) %>% 
  mutate(percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

Extract the posteriors:

```{r}
posts <- posterior_samples(web_mdl_L1Only)
posts <- select(posts,
                b_Intercept:b_trill_real_L1)
```

Make the posteriors into long format for plotting (Figure 3):

```{r}
posts <- pivot_longer(posts,
                      cols = b_Intercept:b_trill_real_L1,
                      names_to = 'coefficient',
                      values_to = 'posterior')
```

Change names for plotting and order:

```{r}
posts <- posts %>%
  mutate(coefficient = ifelse(coefficient == 'b_Intercept', 'Intercept', coefficient),
         coefficient = ifelse(coefficient == 'b_order_num', 'Order', coefficient),
         coefficient = ifelse(coefficient == 'b_r_l_distinction_L1', 'R/L distinction', coefficient),
         coefficient = ifelse(coefficient == 'b_trill_real_L1', '[r] is primary rhotic', coefficient),
         coefficient = factor(coefficient,
                              levels = rev(c('Intercept', 'Order', 'R/L distinction', '[r] is primary rhotic'))))
```

Make a plot of this:

```{r, fig.width = 8, fig.height = 7}
# Aesthetics and geom:

p <- posts %>% ggplot(aes(y = coefficient, x = posterior)) +
  geom_vline(aes(xintercept = 0), linetype = 2) +
  stat_halfeye(alpha = 0.5, fill = 'steelblue')

# Axis labels and coordinates:

p <- p +
  coord_cartesian(xlim = c(-4, +6)) +
  scale_x_continuous(breaks = seq(-4, 6, 1)) +
  xlab('Coefficient') +
  ylab(NULL)

# Tweak cosmetics:

p <- p +
  theme_timo + 
  theme(axis.title.x = element_text(margin = margin(t = 12, b = 0,
                                                    r = 0, l = 0),
                                    face = 'bold', size = 14),
        axis.text.x = element_text(face = 'bold', size = 12),
        axis.text.y = element_text(face = 'bold', size = 12))

# Show and save:

p
ggsave(plot = p, filename = paste0(plots, 'web_mdl_L1Only_coefficients.pdf'),
       width = 8, height = 6)
```

For Figure 4, we want to combine the predictions for each language with
the averages of each language. First, we need to get the predictions
from the model:

```{r}
# Setup data frame with predictors to get predictions for:
newdata <- data.frame(Language = unique(web$Language))
newdata$order_num <- 0
newdata$trill_real_L1 <- web[match(newdata$Language, web$Language), ]$trill_real_L1
newdata$r_l_distinction_L1 <- web[match(newdata$Language, web$Language), ]$r_l_distinction_L1
newdata$Family <- web[match(newdata$Language, web$Language), ]$Family

# Get predictions and append to dataframe:

fit <- fitted(web_mdl_L1Only, newdata = newdata,
              re_formula = NULL, robust = TRUE)
colnames(fit) = c('fit', 'se', 'lwr', 'upr')
newdata <- cbind(newdata, fit)

# Order predictions by descriptive average:

newdata <- arrange(newdata, fit)
newdata <- mutate(newdata,
                  Language = factor(Language, levels = newdata$Language))
```

How many languages are over 0.5? (will be reported in paper)

```{r}
sum(newdata$lwr > 0.5)
```

Finally, add the averages to the plot:

```{r}
newdata$avg <- web_avg[match(newdata$Language, web_avg$Language), ]$M
```

Match language names into there and order:

```{r}
newdata$Language <- web[match(newdata$Language, web$Language), ]$Name
newdata[newdata$Language == 'Chinese', ]$Language <- 'Mandarin Chinese'

newdata <- mutate(newdata,
                  Language = factor(Language, levels = newdata$Language))
```

Convert r_l_distinction_L1 and trill_real_L1 to categorical with 'yes'
and 'no'

```{r}
newdata$r_l_distinction_L1 <- factor(ifelse(newdata$r_l_distinction_L1 == 1, "yes", "no"), 
                                     levels = c("no", "yes"))
newdata$trill_real_L1 <- factor(ifelse(newdata$trill_real_L1 == 1, "yes", "no"), 
                                levels = c("no", "yes"))

```

Setup the plot:

```{r, fig.width = 8, fig.height = 6}
# Aesthetics and geom:

p <- newdata %>% 
  ggplot(aes(x = Language, col = r_l_distinction_L1, y = fit,
             ymin = lwr, ymax = upr, shape = trill_real_L1)) +
  #scale_shape_manual(values = c("no" = 1, "yes" = 3)) +
  geom_errorbar(aes(col = r_l_distinction_L1),
                size = 1.6, width = 0.6) +
  geom_point(size = 9) +
  geom_hline(yintercept = 0.5, linetype = 2, size = 1.5, col = 'grey') + 
  geom_point(aes(y = avg), col = 'black', shape = 23, size = 8.5,
              stroke = 1.5, alpha = 0.8)
  
# Axis labels:

p <- p +
  labs(x = '', y = 'Proportion\nof congruent responses') +
  ggtitle('Posterior medians and descriptive averages of congruent responses\nby language and R/L contrast (color) and [r] as primary rhotic in L1 (shape)') 

# Tweak cosmetics:

p <- p +
  scale_color_manual(values = c(colorBlindBlack8[2], colorBlindBlack8[3])) +
  theme_timo + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, face = 'bold',
                                   size = 30),
        axis.text.y = element_text(face = 'bold', size = 24),
        axis.title = element_text(face = 'bold', size = 40),
        axis.title.y = element_text(face = 'bold', size = 40,
                                    margin = margin(t = 0, r = 35,
                                                    b = 0, l = 0)),
        plot.title = element_text(face = 'bold', size = 40,
                                  margin = margin(t = 0, r = 0,
                                                  b = 30, l = 0)),
        legend.text = element_text(size = 30),
        legend.title = element_blank(),
        legend.position = c(0.95, 0.15),
        legend.justification = c('right', 'bottom'))

# Show and save:

p
ggsave(plot = p, filename = paste0(plots, 'web_mdl_L1Only_by_language.pdf'),
       width = 28, height = 12)
```

### L1 and L2 Only

The model with Order and R/L distinction and alveolar trill as the
preferred variant in L1 and in L2 as fixed effects, as well as by
Language and Family slopes and intercepts.

```{r, message = FALSE, warning = FALSE}
web_mdl_L1L2 <- brm(Match ~ 1 + order_num + r_l_distinction_L1 + trill_real_L1 + 
                      r_l_distinction_L2 + trill_real_L2 + 
                 (1 + order_num|Language) + 
                 (1 + order_num|Family),
                data = web,
                family = bernoulli(link = 'logit'),
                #backend = "cmdstanr",
                seed = 998,
                cores = 4,
                iter = myiter,
                warmup = mywarmup,
                control = list(adapt_delta = 0.995,
                               max_treedepth = 13),
                file = paste0(models, "web_mdl_L1L2.rds"))
```

Show the model:

```{r}
web_mdl_L1L2
```

@Dan, when I included trill_occ_L2, the Rhat was through the roof plus
treedepth errors. So I removed it, because there weren't that many
occurences and I wanted to see. Now without trill_occ_L2, the Rhats are
alright, but the convergence is still poor, treedepth errors. This is
the warning I got: 1: Rows containing NAs were excluded from the model.
2: There were 4959 transitions after warmup that exceeded the maximum
treedepth. Increase max_treedepth above 13. See
<https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded> 3:
Examine the pairs() plot to diagnose sampling problems

And yes, there are a lot of NAs in the L2 data, because not everyone has
an L2. Maybe that is the issue, but I will leave it at that for now.
Perhaps the L1 model is enough.


# Field experiment

## Descriptive statistics

First, how many participants?

```{r}
nrow(field)
```

Sex division

```{r}
table(field$Sex)
```

Ages

```{r}
summary(field$Age)
```

First, how many languages?

```{r}
field %>% count(Language) %>% nrow()
```

Does this number correspond with the L1s?

```{r}
field %>% count(Name) %>% nrow()
```

How many families?

```{r}
field %>% count(Family) %>% nrow()
```

How many have the R/L distinction in the L1 among the languages?

```{r}
field %>% count(Name, r_l_distinction_L1) %>% count(r_l_distinction_L1)
```

How many really use the alveolar trill in L1 among the languages?

```{r}
field %>% count(Name, trill_real_L1) %>% count(trill_real_L1)
```

How many really have the alveolar trill in L1 as an allophone among the
languages?

```{r}
field %>% count(Name, trill_occ_L1) %>% count(trill_occ_L1)
```

What about the same questions for L2. But this will not neatly sum up to
25, due to various possible scenarios for L2 within a specific L1.

How many have the R/L distinction in the L2 among the languages?

```{r}
field %>% count(Name, r_l_distinction_L2) %>% count(r_l_distinction_L2)
```

How many really use the alveolar trill in L2 among the languages?

```{r}
field %>% count(Name, trill_real_L2) %>% count(trill_real_L2)
```

How many really have the alveolar trill in L2 as an allophone among the
languages?

```{r}
field %>% count(Name, trill_occ_L2) %>% count(trill_occ_L2)
```

What is the grand average congruent behavior?

```{r}
mean(field$Match)
```

97%!!!

What about only among those who have L1 without the distinction?

```{r}
field %>%
  filter(r_l_distinction_L1 == "0") %>%
  summarize(mean_match = mean(Match, na.rm = TRUE))
```

100%! WOW.

What about only among those who have L1 without the distinction and no
L2 that distinguishes?

```{r}
field %>%
  filter(r_l_distinction_L1 == "0") %>%
  filter(!EnglishL2YesNo) %>% 
  filter(r_l_distinction_L2 == '0') %>% 
  summarize(mean_match = mean(Match, na.rm = TRUE))
```

There are no such people.

Compute average matching behavior per language and sort:

```{r}
field_avg <- field %>%
  group_by(Language) %>% 
  summarize(M = mean(Match)) %>% 
  arrange(desc(M)) %>% 
  mutate(percent = round(M, 2) * 100,
         percent = str_c(percent, '%'))

# Show:

field_avg %>% print(n = Inf)
```

Check some demographics, also to report in the paper. First, the number
of participants per language:

```{r}
field %>% 
  count(Name, sort = TRUE) %>% print(n = Inf)
```

Then, the number of L1 speakers who have R/L distinction vs. who don't:

```{r}
field %>% count(r_l_distinction_L1) %>%
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

How many people do not have any L2?

```{r}
# raw count of no L2; raw count of all ppl
sum(is.na(field$L2)); nrow(field)

# percentage no L2
sum(is.na(field$L2)) / nrow(field)

# percentage with L2
1 - sum(is.na(field$L2)) / nrow(field)
```

Check how many people knew English as their L2:

```{r}
field %>% count(EnglishL2YesNo) %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

Of those that don't use a R/L distinction in their L1, how many use R/L
distinction in their L2? (double-check if logic alright!)

```{r}
field %>%
  filter(r_l_distinction_L1 == '0') %>% 
  count(r_l_distinction_L2 == '1') %>% 
  mutate(prop = n / sum(n),
         percent = round(prop, 2) * 100,
         percent = str_c(percent, '%'))
```

How many "pure" speakers were there?, i.e., those people that 1) don't
know English, 2) don't use an L1 with a R/L distinction, and 3) don't
know an L2 that distinguishes R/L.

```{r}
field %>% 
  filter(r_l_distinction_L1 == '0') %>%  # excludes English as well
  filter(!EnglishL2YesNo) %>% 
  filter(r_l_distinction_L2 == '0') %>% 
  nrow()
```

None.

## Main model:

Check the distribution of scripts across families to make decisions
about random effects structure:

```{r}
table(field$Family, field$r_l_distinction_L1)
```

@Dan & @Bodo, can you please check if we need some manipulation, like
contrast-coding but weighted? Bodo, I know for bouba/kiki you did some
kind of weighted modification and I'm sure that the proportions of our
predictors are not balanced, see below:

```{r}
field %>% count(r_l_distinction_L1) %>%
  mutate(prop = n / sum(n))
# highly imbalanced

field %>% count(trill_real_L1) %>%
  mutate(prop = n / sum(n))

field %>% count(trill_occ_L1) %>%
  mutate(prop = n / sum(n))
# highly imbalanced

## And for L2, just in case
field %>% count(r_l_distinction_L2) %>%
  mutate(prop = n / sum(n))

field %>% count(trill_real_L2) %>%
  mutate(prop = n / sum(n))

field %>% count(trill_occ_L2) %>%
  mutate(prop = n / sum(n))
```

### L1 Only

The main model with R/L distinction, alveolar trill as the preferred
variant, and allophonic alveolar trill in L1 as fixed effects, as well
as by Language and Family slopes and intercepts.

```{r, message = FALSE, warning = FALSE}
field_mdl_L1Only <- brm(Match ~ 1 + r_l_distinction_L1 + trill_real_L1 + trill_occ_L1 +
                 (1 |Language),
                data = field,
                family = bernoulli(link = 'logit'),
                #backend = "cmdstanr",
                seed = 998,
                cores = 4,
                iter = 10000,
                warmup = 5000,
                control = list(adapt_delta = 0.995,
                               max_treedepth = 15),
                file = paste0(models, "field_mdl_L1Only.rds"))
```

First try with the same parameters as web mdls:

> 1: There were 11534 transitions after warmup that exceeded the maximum
> treedepth. Increase max_treedepth above 13. See
> <https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded>\
> 2: Examine the pairs() plot to diagnose sampling problems\
> 3: The largest R-hat is 2.62, indicating chains have not mixed.
> Running the chains for more iterations may help. See
> <https://mc-stan.org/misc/warnings.html#r-hat>\
> 4: Bulk Effective Samples Size (ESS) is too low, indicating posterior
> means and medians may be unreliable. Running the chains for more
> iterations may help. See
> <https://mc-stan.org/misc/warnings.html#bulk-ess>\
> 5: Tail Effective Samples Size (ESS) is too low, indicating posterior
> variances and tail quantiles may be unreliable. Running the chains for
> more iterations may help. See
> <https://mc-stan.org/misc/warnings.html#tail-ess>

```         
 Family: bernoulli 
  Links: mu = logit 
Formula: Match ~ 1 + r_l_distinction_L1 + trill_real_L1 + trill_occ_L1 + (1 | Language) + (1 | Family) 
   Data: field (Number of observations: 126) 
  Draws: 4 chains, each with iter = 8000; warmup = 4000; thin = 1;
         total post-warmup draws = 16000

Group-Level Effects: 
~Family (Number of levels: 4) 
              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sd(Intercept)     5.54      5.05     0.45    18.49 1.13       29      176

~Language (Number of levels: 6) 
              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sd(Intercept)     1.58      1.34     0.05     5.23 1.10       32       20

Population-Level Effects: 
                    Estimate Est.Error   l-95% CI  u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept             103.33    152.58       2.87    630.55 1.15       18       15
r_l_distinction_L1  56887.37 186857.16 -184696.81 456418.45 2.62        5       16
trill_real_L1          24.79     21.28       0.32     74.78 1.15       18       42
trill_occ_L1       -56994.75 186834.76 -456446.19 184680.49 2.61        5       16

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).
```

Modifications:

-   max_treedepth increased to 15

-   iter increased to 10k

-   warmup increased to 5k

-   removed by-family slope and intercept

Then, it got even worse?

> 1: There were 18766 transitions after warmup that exceeded the maximum
> treedepth. Increase max_treedepth above 15. See
> <https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded>\
> 2: Examine the pairs() plot to diagnose sampling problems\
> 3: The largest R-hat is 3.63, indicating chains have not mixed.
> Running the chains for more iterations may help. See
> <https://mc-stan.org/misc/warnings.html#r-hat>\
> 4: Bulk Effective Samples Size (ESS) is too low, indicating posterior
> means and medians may be unreliable. Running the chains for more
> iterations may help. See
> <https://mc-stan.org/misc/warnings.html#bulk-ess>\
> 5: Tail Effective Samples Size (ESS) is too low, indicating posterior
> variances and tail quantiles may be unreliable. Running the chains for
> more iterations may help. See
> <https://mc-stan.org/misc/warnings.html#tail-ess>

```         
 Family: bernoulli 
  Links: mu = logit 
Formula: Match ~ 1 + r_l_distinction_L1 + trill_real_L1 + trill_occ_L1 + (1 | Language) 
   Data: field (Number of observations: 126) 
  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;
         total post-warmup draws = 20000

Group-Level Effects: 
~Language (Number of levels: 6) 
              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sd(Intercept)     2.63      2.75     0.29    10.87 1.40        9       47

Population-Level Effects: 
                     Estimate  Est.Error    l-95% CI   u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept               38.69      28.66        2.59     106.97 1.07       53      152
r_l_distinction_L1  160913.21 1261024.44 -1752983.46 2301895.06 3.64        4       11
trill_real_L1            6.52       5.77       -0.74      23.36 1.12       26       12
trill_occ_L1       -160950.11 1261029.00 -2301955.01 1752918.28 3.64        4       11

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).
```




Show the model:

```{r}
field_mdl_L1Only
```

